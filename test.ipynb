{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations saved to ./activations/activations.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. 加载 BERT 模型和 tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. 定义一个字典来存储激活值\n",
    "activations = {}\n",
    "\n",
    "# 3. 定义钩子函数来捕获特定位置的激活值\n",
    "def hook_fn(layer_name, target_token_idx):\n",
    "    def hook(module, input, output):\n",
    "        # 获取特定token位置的激活值\n",
    "        # output的形状为 (batch_size, seq_len, hidden_size)\n",
    "        # 我们选定目标token的位置target_token_idx\n",
    "        target_activation = output.detach().cpu().numpy()[:, target_token_idx, :]\n",
    "        activations[layer_name] = target_activation.tolist()\n",
    "    return hook\n",
    "\n",
    "# 4. 注册钩子：遍历所有 Transformer 层并注册钩子\n",
    "hooks = []\n",
    "input_text = \"Hello, how are you?\"\n",
    "\n",
    "# 5. 将输入文本转换为token，并获取目标token的位置（如[CLS]或某个token）\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "# 假设我们想要获取第一个token（[CLS]）的位置\n",
    "target_token_idx = 0  # [CLS]通常是第一个token\n",
    "\n",
    "# 6. 注册钩子到每一层的 intermediate.dense（FFN部分）\n",
    "for i, layer in enumerate(model.encoder.layer):\n",
    "    hook = layer.intermediate.dense.register_forward_hook(hook_fn(f\"layer_{i}_ffn\", target_token_idx))\n",
    "    hooks.append(hook)\n",
    "\n",
    "# 7. 执行前向传播\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 8. 保存激活值到硬盘（保存为 JSON 格式）\n",
    "save_dir = \"./activations\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 保存路径\n",
    "activation_file_path = os.path.join(save_dir, \"activations.json\")\n",
    "\n",
    "# 将激活值字典保存为 JSON 文件\n",
    "with open(activation_file_path, 'w') as json_file:\n",
    "    json.dump(activations, json_file)\n",
    "\n",
    "print(f\"Activations saved to {activation_file_path}\")\n",
    "\n",
    "# 9. 移除钩子\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 3000000/3000000 [00:15<00:00, 191809.91 examples/s]\n",
      "Generating test split: 100%|██████████| 650000/650000 [00:03<00:00, 193685.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"nyu-mll/glue\",\"sst2\")\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes\")\n",
    "# dataset = load_dataset(\"fancyzhx/ag_news\")\n",
    "# dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "# ds = load_dataset(\"codyburker/yelp_review_sampled\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "# ds = load_dataset(\"Yelp/yelp_review_full\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\", num_labels=5, cache_dir=\"/cache/huggingface/hub\"\n",
    "# )\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# ds = load_dataset(\"bookcorpus/bookcorpus\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "# ds = load_dataset(\"bookcorpus/bookcorpus\", cache_dir=\"/cache/huggingface/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 示例 Counter 对象\n",
    "data = Counter({'A': 10, 'B': 15, 'C': 5, 'D': 20})\n",
    "\n",
    "# 绘制直方图\n",
    "def plot_counter_histogram(counter_obj):\n",
    "    labels = list(counter_obj.keys())\n",
    "    values = list(counter_obj.values())\n",
    "\n",
    "    plt.bar(labels, values, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Histogram of Counter')\n",
    "    plt.show()\n",
    "\n",
    "# 调用函数绘制\n",
    "plot_counter_histogram(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.7593, -0.1422, -0.8351, -0.6397, -0.1545, -1.1371, -0.7905,  0.8253,\n",
      "          1.0648,  0.4474],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个 Linear 层\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# 初始化输入\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 前向计算\n",
    "output = linear(x)\n",
    "\n",
    "# 钩子函数：仅保留第 i 个神经元的梯度\n",
    "i = 2\n",
    "def hook_fn(grad):\n",
    "    mask = torch.zeros_like(grad)\n",
    "    mask[i, :] = 1  # 仅保留第 i 行的梯度\n",
    "    return grad * mask\n",
    "\n",
    "linear.weight.register_hook(hook_fn)\n",
    "\n",
    "# 损失函数\n",
    "loss = output.sum()\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(linear.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 2.6.0a0+df5bbc09d1.nv24.11\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.6\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 24.04.1 LTS (x86_64)\n",
      "GCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.31.0\n",
      "Libc version: glibc-2.39\n",
      "\n",
      "Python version: 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.39\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\n",
      "Nvidia driver version: 560.35.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.1\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               40\n",
      "On-line CPU(s) list:                  0-39\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   10\n",
      "Socket(s):                            2\n",
      "Stepping:                             7\n",
      "CPU(s) scaling MHz:                   42%\n",
      "CPU max MHz:                          3200.0000\n",
      "CPU min MHz:                          1000.0000\n",
      "BogoMIPS:                             4400.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts vnmi pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            640 KiB (20 instances)\n",
      "L1i cache:                            640 KiB (20 instances)\n",
      "L2 cache:                             20 MiB (20 instances)\n",
      "L3 cache:                             27.5 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0-9,20-29\n",
      "NUMA node1 CPU(s):                    10-19,30-39\n",
      "Vulnerability Gather data sampling:   Mitigation; Microcode\n",
      "Vulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Mitigation; Enhanced IBRS\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Mitigation; TSX disabled\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] onnx==1.17.0\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] pytorch-triton==3.0.0+72734f086\n",
      "[pip3] torch==2.6.0a0+df5bbc09d1.nv24.11\n",
      "[pip3] torch-tb-profiler==0.4.3\n",
      "[pip3] torch_tensorrt==2.6.0a0\n",
      "[pip3] torchprofile==0.0.4\n",
      "[pip3] torchvision==0.20.0a0\n",
      "[conda] Could not collect\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.collect_env import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 841374.34 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 139139.96 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 212315.22 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels range: min = 0, max = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "print(f\"Train labels range: min = {min(train_labels)}, max = {max(train_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 302\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m    299\u001b[0m     pprint(model)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtest_custom_bert_for_masked_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 285\u001b[0m, in \u001b[0;36mtest_custom_bert_for_masked_lm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Compare the logits before and after the modification\u001b[39;00m\n\u001b[1;32m    284\u001b[0m difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(original_mask_logits \u001b[38;5;241m-\u001b[39m modified_mask_logits)\n\u001b[0;32m--> 285\u001b[0m forward_with_partitioning \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_partitioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal logits for [MASK] position:\u001b[39m\u001b[38;5;124m\"\u001b[39m, original_mask_logits)\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m, in \u001b[0;36mCustomBertForMaskedLM.forward_with_partitioning\u001b[0;34m(self, target_position)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodify_ffn_activation(\n\u001b[1;32m     67\u001b[0m         idx, target_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partitioning_activations[idx]\n\u001b[1;32m     68\u001b[0m     )\n\u001b[0;32m---> 69\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partitioning_logits\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     71\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     72\u001b[0m     )  \u001b[38;5;66;03m# logits不可使用detach，否则会导致梯度丢失\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:1461\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1461\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1476\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py:258\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1803\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1801\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1803\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1806\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[12], line 152\u001b[0m, in \u001b[0;36mCustomBertForMaskedLM.modify_ffn_activation.<locals>.hook_fn\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_activations\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Ensure the shape matches\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m batch_idx, seq_idx \u001b[38;5;241m=\u001b[39m target_position\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_activation\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m output[batch_idx, seq_idx]\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[batch_idx,\u001b[38;5;250m \u001b[39mseq_idx]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_activation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomBertForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self._intermediate_activations = []\n",
    "        self._original_activations = []\n",
    "        self._mask_logits = None\n",
    "        self._partitioning_activations = []\n",
    "        self._partitioning_step = []\n",
    "        self._partitioning_logits = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        保存现有参数以用于forward_with_partitioning\n",
    "        \"\"\"\n",
    "        self._args = args\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "        # Hook to capture intermediate activations\n",
    "        def hook_fn(module, input, output):\n",
    "            # 如果 _intermediate_activations 不为空，则清空它\n",
    "            if len(self._intermediate_activations) == self.config.num_hidden_layers:\n",
    "                self._intermediate_activations.clear()\n",
    "\n",
    "            # 添加新的激活到 _intermediate_activations\n",
    "            self._intermediate_activations.append(output.detach().cpu())\n",
    "\n",
    "        # Register hooks on intermediate layers\n",
    "        hooks = []\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            hooks.append(layer.intermediate.register_forward_hook(hook_fn))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        # Remove hooks after forward pass\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward_with_partitioning(self, target_position=3):\n",
    "        \"\"\"\n",
    "        target_pos token idx\n",
    "        \"\"\"\n",
    "        for layer in self._intermediate_activations:\n",
    "            target_vector = layer[:, target_position, :]\n",
    "            partitioning, step = self.generate_partitioning(target_vector)\n",
    "            self._partitioning_activations.append(partitioning.detach().cpu())\n",
    "            self._partitioning_step.append(step.detach().cpu())\n",
    "\n",
    "        # Forward pass\n",
    "        for idx in range(self.config.num_hidden_layers):\n",
    "            self.modify_ffn_activation(\n",
    "                idx, target_position, self._partitioning_activations[idx]\n",
    "            )\n",
    "            outputs = super().forward(*self._args, **self._kwargs)\n",
    "            self._partitioning_logits.append(\n",
    "                outputs.logits\n",
    "            )  # logits不可使用detach，否则会导致梯度丢失\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_mask_logits(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Returns logits at [MASK] positions for the given input.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure model is in eval mode\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Find [MASK] token positions (token_id=103 for BERT by default)\n",
    "        mask_token_id = 103\n",
    "        # mask_token_id = self.config.mask_token_id\n",
    "        mask_positions = input_ids == mask_token_id\n",
    "\n",
    "        # Extract logits for [MASK] positions\n",
    "        mask_logits = logits[mask_positions]\n",
    "\n",
    "        return mask_logits\n",
    "\n",
    "    @property\n",
    "    def get_intermediate_activations(self):\n",
    "        \"\"\"\n",
    "        Returns the intermediate neuron activations from the last forward pass.\n",
    "        Raises an error if intermediate_activations is empty.\n",
    "        \"\"\"\n",
    "        if not self._intermediate_activations:\n",
    "            raise ValueError(\n",
    "                \"Intermediate activations are empty. Ensure that a forward pass has been performed.\"\n",
    "            )\n",
    "        return self._intermediate_activations\n",
    "\n",
    "    @property\n",
    "    def original_activations(self):\n",
    "        \"\"\"\n",
    "        Returns the original (before modification) intermediate neuron activations.\n",
    "        \"\"\"\n",
    "        if not self._original_activations:\n",
    "            raise ValueError(\n",
    "                \"Original activations are empty. Ensure that a forward pass has been performed.\"\n",
    "            )\n",
    "        return self._original_activations\n",
    "\n",
    "    @property\n",
    "    def mask_logits(self):\n",
    "        \"\"\"\n",
    "        Returns the logits at [MASK] positions from the last forward pass.\n",
    "        Raises an error if _mask_logits is None.\n",
    "        \"\"\"\n",
    "        if self._mask_logits is None:\n",
    "            raise ValueError(\n",
    "                \"Mask logits are not available. Ensure that a forward pass has been performed with [MASK] tokens.\"\n",
    "            )\n",
    "        return self._mask_logits\n",
    "\n",
    "    def modify_ffn_activation(self, layer_idx, target_position, new_activation):\n",
    "        \"\"\"\n",
    "        Modifies the hidden activations of a specific FFN layer at a specific position in the model.\n",
    "        Stores the original activations before modifying.\n",
    "\n",
    "        Args:\n",
    "            layer_idx (int): Index of the transformer layer to modify (0-indexed).\n",
    "            target_position (tuple): A tuple specifying the target position (batch_idx, seq_idx).\n",
    "            new_activation (torch.Tensor): The new activation values with shape matching the FFN layer output\n",
    "                                            (e.g., [intermediate_size]).\n",
    "        \"\"\"\n",
    "        if not isinstance(new_activation, torch.Tensor):\n",
    "            raise ValueError(\"new_activation must be a torch.Tensor.\")\n",
    "\n",
    "        def hook_fn(module, input, output):\n",
    "            # Store original activations before modification\n",
    "            self._original_activations.append(output.detach().cpu())\n",
    "\n",
    "            # Ensure the shape matches\n",
    "            batch_idx, seq_idx = target_position\n",
    "            if new_activation.shape != output[batch_idx, seq_idx].shape:\n",
    "                raise ValueError(\n",
    "                    f\"Shape mismatch: Expected {output[batch_idx, seq_idx].shape}, \"\n",
    "                    f\"but got {new_activation.shape}.\"\n",
    "                )\n",
    "\n",
    "            # Modify the activation at the target position\n",
    "            output = output.clone()  # Clone to avoid in-place modification\n",
    "            output[batch_idx, seq_idx] = new_activation\n",
    "            return output\n",
    "\n",
    "        # Register the hook\n",
    "        hook = self.bert.encoder.layer[layer_idx].intermediate.register_forward_hook(\n",
    "            hook_fn\n",
    "        )\n",
    "        \"\"\"\n",
    "        这个函数利用hook修改指定位置上的FFN层的激活值，并存储修改前的激活值。\n",
    "        \"\"\"\n",
    "        return hook  # Return the hook handle for later removal if needed\n",
    "\n",
    "    def generate_partitioning(self, vector, times=20):\n",
    "        \"\"\"\n",
    "        生成一组按比例缩放的输入数据。\n",
    "\n",
    "        参数:\n",
    "            emb (torch.Tensor): 输入张量，形状为 (1, ffn_size)，表示一个基准向量。\n",
    "            batch_size (int): 每个批次中的样本数量。\n",
    "            num_batch (int): 批次的总数量。\n",
    "\n",
    "        返回:\n",
    "            partitioning (torch.Tensor): 形状为 (batch_size * num_batch, ffn_size) 的张量，包含生成的缩放输入。\n",
    "            step (torch.Tensor): 每次增量的大小，形状为 (1, ffn_size)，表示每一步的变化量。\n",
    "        equal partitioning\n",
    "\n",
    "        \"\"\"\n",
    "        vector = vector\n",
    "        # vector = vector.unsqueeze(0)\n",
    "        baseline = torch.zeros_like(vector)  # (1, ffn_size)\n",
    "\n",
    "        # step: 计算每一步的增量，即 emb 和 baseline 之间的差距除以总的样本数。\n",
    "        # 这是为了确保从 baseline 到 emb 的变化是均匀的，按比例分配给每个样本。\n",
    "        step = (vector - baseline) / times  # (1, ffn_size) # 每个分量上的间隔值\n",
    "\n",
    "        # res: 使用 baseline 和 step 生成 scaled input 的列表。\n",
    "        # 通过 torch.add 和 step * i 逐步构建每个样本的值，并将这些值沿着第一个维度 (batch_size * num_batch) 拼接。\n",
    "        partitioning = torch.cat(\n",
    "            [torch.add(baseline, step * i) for i in range(times)], dim=0\n",
    "        )  # (step, ffn_size)\n",
    "\n",
    "        # 返回生成的输入数据和每一步的增量。\n",
    "        return partitioning, step[0]\n",
    "\n",
    "    # @staticmethod\n",
    "    def calulate_integrated_gradients(self, logits, target_label, partitioning, step):\n",
    "\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        o = torch.unbind(prob[:, target_label])\n",
    "        gradient = torch.autograd.grad(o, partitioning).sequeeze(0)\n",
    "        grad_summed = gradient.sum(dim=0)  # (ffn_size)\n",
    "        ig_pred = grad_summed * step  # (ffn_size)\n",
    "        return ig_pred\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming CustomBertForMaskedLM is defined as per the provided code\n",
    "\n",
    "\n",
    "def test_custom_bert_for_masked_lm():\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "    # Instantiate the model and move it to the appropriate device (CPU or GPU)\n",
    "    model = CustomBertForMaskedLM(config).to(device)\n",
    "\n",
    "    # Load pre-trained weights into the custom model\n",
    "    model.bert.load_state_dict(\n",
    "        BertForMaskedLM.from_pretrained(model_name).bert.state_dict()\n",
    "    )\n",
    "\n",
    "    # Tokenize some example input text with a [MASK] token\n",
    "    text = \"The quick brown fox jumps over the [MASK] dog.\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Perform the first forward pass to get original logits (before modification)\n",
    "    original_outputs = model(\n",
    "        input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "    )\n",
    "    original_logits = original_outputs.logits\n",
    "\n",
    "    # Get the original logits for the [MASK] position\n",
    "    mask_token_id = 103\n",
    "    mask_positions = input_ids == mask_token_id\n",
    "    original_mask_logits = original_logits[mask_positions]\n",
    "\n",
    "    # Modify the activations of a specific FFN layer and position\n",
    "    target_position = (\n",
    "        0,\n",
    "        5,\n",
    "    )  # Example: batch_idx = 0, seq_idx = 5 (corresponding to the [MASK] token)\n",
    "    new_activation = torch.randn(model.config.intermediate_size).to(\n",
    "        device\n",
    "    )  # Random tensor of appropriate size\n",
    "\n",
    "    # Modify the activation in the 3rd transformer layer\n",
    "    hook = model.modify_ffn_activation(\n",
    "        layer_idx=2, target_position=target_position, new_activation=new_activation\n",
    "    )\n",
    "\n",
    "    # Perform the second forward pass after modification\n",
    "    modified_outputs = model(\n",
    "        input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "    )\n",
    "    modified_logits = modified_outputs.logits\n",
    "\n",
    "    # Get the modified logits for the [MASK] position\n",
    "    modified_mask_logits = modified_logits[mask_positions]\n",
    "\n",
    "    # Compare the logits before and after the modification\n",
    "    difference = torch.abs(original_mask_logits - modified_mask_logits)\n",
    "    forward_with_partitioning = model.forward_with_partitioning(target_position=5)\n",
    "    # Print the results\n",
    "    print(\"Original logits for [MASK] position:\", original_mask_logits)\n",
    "    print(\"Modified logits for [MASK] position:\", modified_mask_logits)\n",
    "    print(\"Difference between original and modified logits:\", difference)\n",
    "    print(f\"Maximum difference: {difference.max()}\")  # Print the maximum difference\n",
    "    partitioning, step = model.generate_partitioning(new_activation, times=20)\n",
    "    re = model.calulate_integrated_gradients(\n",
    "        original_logits, target_label=0, partitioning=partitioning, step=step\n",
    "    )\n",
    "    # Optionally remove the hook after testing\n",
    "    hook.remove()\n",
    "    from pprint import pprint\n",
    "\n",
    "    pprint(model)\n",
    "\n",
    "\n",
    "test_custom_bert_for_masked_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 打印显存统计信息\n",
    "print(torch.cuda.memory_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 24588.08 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:00<00:00, 27279.22 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:01<00:00, 27091.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "dataset = load_dataset(\"imdb\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "new_class_labels = ClassLabel(\n",
    "    num_classes=6, names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\", \"neg\", \"pos\"]\n",
    ")\n",
    "dataset = dataset.cast_column(\"label\", new_class_labels)\n",
    "def update_label(example):\n",
    "        original_label = example[\"label\"]\n",
    "\n",
    "        # 重新映射原始标签d\n",
    "        if original_label == 0:\n",
    "            example[\"label\"] = 4  # 原标签 0 -> 新标签 3\n",
    "        elif original_label == 1:\n",
    "            example[\"label\"] = 5  # 原标签 1 -> 新标签 4\n",
    "\n",
    "        return example\n",
    "\n",
    "dataset = dataset.map(update_label)\n",
    "print(dataset[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random data saved to random_data_10.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Function to generate random data\n",
    "def generate_random_data(num_entries):\n",
    "    data = [[random.randint(0, 11), random.randint(0, 3071)] for _ in range(num_entries)]\n",
    "    return data\n",
    "\n",
    "# Generate data with a specified number of entries\n",
    "num_entries = 10\n",
    "random_data = generate_random_data(num_entries)\n",
    "\n",
    "# Save data to a JSON file\n",
    "output_file = f'random_data_{num_entries}.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(random_data, f, indent=4)\n",
    "\n",
    "print(f\"Random data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/3c/fc/3cfce2480f0afb774db7b6c4dd145bec956c6a8bcbd623101f0f0d7b512366e4/3c900ade9ea2c83255b09021e70d80d58b933880655fc6e794e12a652a30d33f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27top4-balanced.csv%3B+filename%3D%22top4-balanced.csv%22%3B&response-content-type=text%2Fcsv&Expires=1737464191&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQ2NDE5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzNjL2ZjLzNjZmNlMjQ4MGYwYWZiNzc0ZGI3YjZjNGRkMTQ1YmVjOTU2YzZhOGJjYmQ2MjMxMDFmMGYwZDdiNTEyMzY2ZTQvM2M5MDBhZGU5ZWEyYzgzMjU1YjA5MDIxZTcwZDgwZDU4YjkzMzg4MDY1NWZjNmU3OTRlMTJhNjUyYTMwZDMzZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=S0RA0l1d%7EYlsnN6s5k8GosEVY2xKovrAblP8htJnar9R2i3py7AOYfu-Pi2MSHh9Em7-m5Sfex8Kzlc49hjoSEHA4lnYdnH%7ENeEUBNCEGN7fCxvdAlczZhn8YPwxQStCg-tW8eL6jz3kl1jPT0kOaBsBg1Lgc0LpgmYHsw27-ogQhxsvSPP3MtVR--%7EJTtwqT5xxcxjAQmFc%7EpKNGl8nBfQyrlmFBF9PMB6M8wZTi81tHqT1f0eGKpytTStSjfBNjiLSPZ56nvErsZG7lX0hs3UPgW3qdTZ-7GbbHanMvNAYlQKnVEPdYQ9eaoLtuwEB01XjKCBk2VlMZGklc-NKSw__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 936\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:813\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 813\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp_closed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:715\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py:826\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontemmcm/ag_news\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop4-balanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py:2151\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2161\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2162\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/builder.py:978\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    977\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 978\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/csv/csv.py:156\u001b[0m, in \u001b[0;36mCsv._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m dl_manager\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mextract_on_the_fly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m splits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_name, files \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/download/download_manager.py:326\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/download/download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py:399\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    396\u001b[0m         k: _single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[1;32m    397\u001b[0m     }\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/download/download_manager.py:220\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/download/download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[1;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py:188\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mHfFileSystem(\n\u001b[1;32m    179\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT, token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m    180\u001b[0m )\u001b[38;5;241m.\u001b[39mresolve_path(url_or_filename)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_datasets_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    197\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRepositoryNotFoundError,\n\u001b[1;32m    198\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mEntryNotFoundError,\n\u001b[1;32m    199\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRevisionNotFoundError,\n\u001b[1;32m    200\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGatedRepoError,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py:5252\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5249\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 5252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5264\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5268\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:469\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    467\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    468\u001b[0m         reset_sessions()  \u001b[38;5;66;03m# In case of SSLError it's best to reset the shared requests.Session objects\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_resume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_nb_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_nb_retries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m         consistency_error_message\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    483\u001b[0m             actual_size\u001b[38;5;241m=\u001b[39mtemp_file\u001b[38;5;241m.\u001b[39mtell(),\n\u001b[1;32m    484\u001b[0m         )\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 936\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    939\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"contemmcm/ag_news\", \"top4-balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 120000/120000 [00:00<00:00, 236576.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 90000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5700\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 90000/90000 [00:00<00:00, 229994.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5700/5700 [00:00<00:00, 220520.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"fancyzhx/ag_news\", cache_dir=\"/cache/huggingface/datasets\"\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "# target_labels = [\"World\", \"Sports\", \"Business\"]  # 例：指定要保留的标签\n",
    "target_labels = [0, 1, 2]  # 例：指定要保留的标签\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] in target_labels)\n",
    "print(dataset)\n",
    "dataset.save_to_disk(\"/cache/huggingface/datasets/agnews_3labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2786\n",
      "})\n",
      "{'text': 'An international team of researchers with key participation from the PRISMA+Cluster of Excellence at Johannes Gutenberg University Mainz (JGU) and the Helmholtz Institute Mainz (HIM) has published for the first time comprehensive data on the search for dark matter using a worldwide network of optical magnetometers. According to the scientists, dark matter fields should produce a characteristic signal pattern that can be detected by correlated measurements at multiple stations of the GNOME network. Analysis of data from a one-month continuous GNOME operation has not yet yielded a corresponding indication. However, the measurement allows the formulation of constraints on the characteristics of dark matter, as the researchers report in the journal Nature Physics. GNOME stands for Global Network of Optical Magnetometers for Exotic Physics Searches. Behind it are magnetometers distributed around the world in Germany, Serbia, Poland, Israel, South Korea, China, Australia, and the United States. With GNOME, the researchers particularly want to advance the search for dark matter—one of the most exciting challenges of fundamental physics in the 21st century. After all, it has long been known that many puzzling astronomical observations, such as the rotation speed of stars in galaxies or the spectrum of the cosmic background radiation, can best be explained by dark matter. \"Extremely light bosonic particles are considered one of the most promising candidates for dark matter today. These include so-called axion-like particles—ALPs for short,\" said ProfessorDr. Dmitry Budker, professor at PRISMA+and at HIM, an institutional collaboration of Johannes Gutenberg University Mainz and the GSI Helmholtzzentrum für Schwerionenforschung in Darmstadt. \"They can also be considered as a classical field oscillating with a certain frequency. A peculiarity of such bosonic fields is that—according to a possible theoretical scenario—they can form patterns and structures. As a result, the density of dark matter could be concentrated in many different regions—discrete domain walls smaller than a galaxy but much larger than Earth could form, for example.\" \"If such a wall encounters the Earth, it is gradually detected by the GNOME network and can cause transient characteristic signal patterns in the magnetometers,\" explained Dr. Arne Wickenbrock, one of the study\\'s co-authors. \"Even more, the signals are correlated with each other in certain ways—depending on how fast the wall is moving and when it reaches each location.\" The network meanwhile consists of 14 magnetometers distributed over eight countries worldwide. Nine of them provided data for the current analysis. The measurement principle is based on an interaction of dark matter with the nuclear spins of the atoms in the magnetometer. The atoms are excited with a laser at a specific frequency, orienting the nuclear spins in one direction. A potential dark matter field can disturb this direction, which is measurable. Figuratively speaking, one can imagine that the atoms in the magnetometer initially dance around in confusion, as clarified byHector Masia-Roig, a doctoral student in the Budker group and also an author of the current study. \"When they \\'hear\\' the right frequency of laser light, they all spin together. Dark matter particles can throw the dancing atoms out of balance. We can measure this perturbation very precisely.\" Now the network of magnetometers becomes important: When the Earth moves through a spatially limited wall of dark matter, the dancing atoms in all stations are gradually disturbed.One of these stations is located in a laboratory at the Helmholtz Institute in Mainz. \"Only when we match the signals from all the stations can we assess what triggered the disturbance,\"said Masia-Roig. \"Applied to the image of the dancing atoms, this means: If we compare the measurement results from all the stations, we can decide whether it was just one brave dancer dancing out of line or actually a global dark matter disturbance.\" In the current study, the research team analyzes data from a one-month continuous operation of GNOME. The result: Statistically significant signals did not appear in the investigated mass range from one femtoelectronvolt (feV) to 100,000 feV. Conversely, this means that the researchers can narrow down the range in which such signals could theoretically be found even further than before. For scenarios that rely on discrete dark matter walls, this is an important result—\"even though we have not yet been able to detect such a domain wall with our global ring search,\" added Joseph Smiga, another Ph.D. student in Mainz and author of the study. Future work of the GNOME collaboration will focus on improving both the magnetometers themselves and the data analysis. In particular, continuous operation should be even more stable. This is important to reliably search for signals that last longer than an hour. In addition, the previous alkali atoms in the magnetometers are to be replaced by noble gasses. Under the title Advanced GNOME, the researchers expect this to result in considerably better sensitivity for future measurements in the search for ALPs and dark matter. ', 'label': 'Physics'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/cache/huggingface/datasets/SciNews_3labels\")[\"test\"]\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 33497\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 22873/22873 [00:00<00:00, 44621.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2868/2868 [00:00<00:00, 45658.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2786/2786 [00:00<00:00, 50791.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'When cancer spreads to another organ, it most commonly moves to the liver, and now researchers at the Abramson Cancer Center of the University of Pennsylvania say they know why. A new study, published today in Nature, shows hepatocytes—the chief functional cells of the liver—are at the center of a chain reaction that makes it particularly susceptible to cancer cells. These hepatocytes respond to inflammation by activating a protein called STAT3, which in turn increases their production of other proteins called SAA, which then remodel the liver and create the \"soil\" needed for cancer cells to \"seed.\" The researchers show that stopping this process by using antibodies that block IL-6—the inflammatory signal that drives this chain reaction—can limit the potential of cancer to spread to the liver. \"The seed-and-soil hypothesis is well-recognized, but our research now shows that hepatocytes are the major orchestrators of this process,\" said senior author Gregory L. Beatty, MD, Ph.D., an assistant professor of Hematology-Oncology at Penn\\'s Perelman School of Medicine. Jae W. Lee, an MD/Ph.D. candidate in Beatty\\'s laboratory, is the lead author. For this study, the team first used mouse models of pancreatic ductal adenocarcinoma (PDAC), the most common type of pancreatic cancer and currently the third leading cause of cancer death in the United States. They found that nearly all hepatocytes showed STAT3 activation in mice with cancer, compared to less than two percent of hepatocytes in mice without tumors. They then partnered with investigators at the Mayo Clinic Arizona and other Penn colleagues to show that this same biology could be seen in patients with pancreatic cancer as well colon and lung cancer. Genetically deleting STAT3 only in hepatocytes effectively blocked the increased susceptibility of the liver to cancer seeding in mice. The team collaborated further with investigators at the University of Kentucky to show that IL-6 controls STAT3 signaling in these cells and instructs hepatocytes to make SAA, which acts as an alarm to attract inflammatory cells and initiate a fibrotic reaction that together establish the \"soil.\" \"The liver is an important sensor in the body,\" Lee said. \"We show that hepatocytes sense inflammation and respond in a structured way that cancer uses to help it spread.\" The study also found that IL-6 drives changes in the liver whether there\\'s a tumor present or not, implying that any condition associated with increased IL-6 levels—such as obesity or cardiovascular disease, among others—could affect the liver\\'s receptiveness to cancer. Researchers say this provides evidence that therapies which target hepatocytes may be able to prevent cancer from spreading to the liver, a major cause of cancer mortality. ', 'label': 'Medicine'}\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 33497\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "ds = load_dataset(\"dongqi-me/SciNews\")\n",
    "target_labels = [\"Physics\", \"Medicine\", \"Biology\"]  # 例：指定要保留的标签\n",
    "ds = ds.select_columns([\"News_Body\", \"Topic\"])\n",
    "ds = ds.rename_columns({\"News_Body\": \"text\", \"Topic\": \"label\"} )\n",
    "print(ds[\"train\"])\n",
    "\n",
    "# 过滤数据集，保留 'Topic' 中属于 target_labels 的行\n",
    "filtered_ds = ds.filter(lambda x: x[\"label\"] in target_labels)\n",
    "filtered_ds.save_to_disk(\"/cache/huggingface/datasets/SciNews_3labels\")\n",
    "\n",
    "# 输出结果\n",
    "print(filtered_ds[\"train\"][0])\n",
    "print(ds[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"/root/ftg/results/new_6tags_agnews_based_imdb\")\n",
    "peft_model_id = \"/root/ftg/results/imdb_01_17_10:18/checkpoint-4689\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"model_id\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "Question: Who predicted Hawking radiation?\n",
      "Answer: stephen hawking\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 示例上下文和问题\n",
    "context = \"\"\"Hawking radiation is theoretical radiation that is predicted to be emitted by black holes. It is named after physicist Stephen Hawking, who predicted it in 1974. This radiation is a result of quantum mechanical effects near the event horizon of a black hole.\"\"\"\n",
    "question = \"Who predicted Hawking radiation?\"\n",
    "\n",
    "# 分词并生成模型输入\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 获取答案的起始和结束位置\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits)\n",
    "\n",
    "# 解码出答案\n",
    "answer_tokens = inputs.input_ids[0][start_idx:end_idx+1]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(model)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
