{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations saved to ./activations/activations.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. 加载 BERT 模型和 tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. 定义一个字典来存储激活值\n",
    "activations = {}\n",
    "\n",
    "# 3. 定义钩子函数来捕获特定位置的激活值\n",
    "def hook_fn(layer_name, target_token_idx):\n",
    "    def hook(module, input, output):\n",
    "        # 获取特定token位置的激活值\n",
    "        # output的形状为 (batch_size, seq_len, hidden_size)\n",
    "        # 我们选定目标token的位置target_token_idx\n",
    "        target_activation = output.detach().cpu().numpy()[:, target_token_idx, :]\n",
    "        activations[layer_name] = target_activation.tolist()\n",
    "    return hook\n",
    "\n",
    "# 4. 注册钩子：遍历所有 Transformer 层并注册钩子\n",
    "hooks = []\n",
    "input_text = \"Hello, how are you?\"\n",
    "\n",
    "# 5. 将输入文本转换为token，并获取目标token的位置（如[CLS]或某个token）\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "# 假设我们想要获取第一个token（[CLS]）的位置\n",
    "target_token_idx = 0  # [CLS]通常是第一个token\n",
    "\n",
    "# 6. 注册钩子到每一层的 intermediate.dense（FFN部分）\n",
    "for i, layer in enumerate(model.encoder.layer):\n",
    "    hook = layer.intermediate.dense.register_forward_hook(hook_fn(f\"layer_{i}_ffn\", target_token_idx))\n",
    "    hooks.append(hook)\n",
    "\n",
    "# 7. 执行前向传播\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 8. 保存激活值到硬盘（保存为 JSON 格式）\n",
    "save_dir = \"./activations\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 保存路径\n",
    "activation_file_path = os.path.join(save_dir, \"activations.json\")\n",
    "\n",
    "# 将激活值字典保存为 JSON 文件\n",
    "with open(activation_file_path, 'w') as json_file:\n",
    "    json.dump(activations, json_file)\n",
    "\n",
    "print(f\"Activations saved to {activation_file_path}\")\n",
    "\n",
    "# 9. 移除钩子\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 3000000/3000000 [00:15<00:00, 191809.91 examples/s]\n",
      "Generating test split: 100%|██████████| 650000/650000 [00:03<00:00, 193685.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"nyu-mll/glue\",\"sst2\")\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes\")\n",
    "# dataset = load_dataset(\"fancyzhx/ag_news\")\n",
    "# dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "# ds = load_dataset(\"codyburker/yelp_review_sampled\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "# ds = load_dataset(\"Yelp/yelp_review_full\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\", num_labels=5, cache_dir=\"/cache/huggingface/hub\"\n",
    "# )\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# ds = load_dataset(\"bookcorpus/bookcorpus\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "# ds = load_dataset(\"bookcorpus/bookcorpus\", cache_dir=\"/cache/huggingface/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 示例 Counter 对象\n",
    "data = Counter({'A': 10, 'B': 15, 'C': 5, 'D': 20})\n",
    "\n",
    "# 绘制直方图\n",
    "def plot_counter_histogram(counter_obj):\n",
    "    labels = list(counter_obj.keys())\n",
    "    values = list(counter_obj.values())\n",
    "\n",
    "    plt.bar(labels, values, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Histogram of Counter')\n",
    "    plt.show()\n",
    "\n",
    "# 调用函数绘制\n",
    "plot_counter_histogram(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.7593, -0.1422, -0.8351, -0.6397, -0.1545, -1.1371, -0.7905,  0.8253,\n",
      "          1.0648,  0.4474],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个 Linear 层\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# 初始化输入\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 前向计算\n",
    "output = linear(x)\n",
    "\n",
    "# 钩子函数：仅保留第 i 个神经元的梯度\n",
    "i = 2\n",
    "def hook_fn(grad):\n",
    "    mask = torch.zeros_like(grad)\n",
    "    mask[i, :] = 1  # 仅保留第 i 行的梯度\n",
    "    return grad * mask\n",
    "\n",
    "linear.weight.register_hook(hook_fn)\n",
    "\n",
    "# 损失函数\n",
    "loss = output.sum()\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(linear.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 2.6.0a0+df5bbc09d1.nv24.11\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.6\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 24.04.1 LTS (x86_64)\n",
      "GCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.31.0\n",
      "Libc version: glibc-2.39\n",
      "\n",
      "Python version: 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.39\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\n",
      "Nvidia driver version: 560.35.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.1\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.1\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               40\n",
      "On-line CPU(s) list:                  0-39\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   10\n",
      "Socket(s):                            2\n",
      "Stepping:                             7\n",
      "CPU(s) scaling MHz:                   42%\n",
      "CPU max MHz:                          3200.0000\n",
      "CPU min MHz:                          1000.0000\n",
      "BogoMIPS:                             4400.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts vnmi pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            640 KiB (20 instances)\n",
      "L1i cache:                            640 KiB (20 instances)\n",
      "L2 cache:                             20 MiB (20 instances)\n",
      "L3 cache:                             27.5 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0-9,20-29\n",
      "NUMA node1 CPU(s):                    10-19,30-39\n",
      "Vulnerability Gather data sampling:   Mitigation; Microcode\n",
      "Vulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Mitigation; Enhanced IBRS\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Mitigation; TSX disabled\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] onnx==1.17.0\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] pytorch-triton==3.0.0+72734f086\n",
      "[pip3] torch==2.6.0a0+df5bbc09d1.nv24.11\n",
      "[pip3] torch-tb-profiler==0.4.3\n",
      "[pip3] torch_tensorrt==2.6.0a0\n",
      "[pip3] torchprofile==0.0.4\n",
      "[pip3] torchvision==0.20.0a0\n",
      "[conda] Could not collect\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.collect_env import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 841374.34 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 139139.96 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 212315.22 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels range: min = 0, max = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "print(f\"Train labels range: min = {min(train_labels)}, max = {max(train_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 302\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m    299\u001b[0m     pprint(model)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtest_custom_bert_for_masked_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 285\u001b[0m, in \u001b[0;36mtest_custom_bert_for_masked_lm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Compare the logits before and after the modification\u001b[39;00m\n\u001b[1;32m    284\u001b[0m difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(original_mask_logits \u001b[38;5;241m-\u001b[39m modified_mask_logits)\n\u001b[0;32m--> 285\u001b[0m forward_with_partitioning \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_partitioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal logits for [MASK] position:\u001b[39m\u001b[38;5;124m\"\u001b[39m, original_mask_logits)\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m, in \u001b[0;36mCustomBertForMaskedLM.forward_with_partitioning\u001b[0;34m(self, target_position)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodify_ffn_activation(\n\u001b[1;32m     67\u001b[0m         idx, target_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partitioning_activations[idx]\n\u001b[1;32m     68\u001b[0m     )\n\u001b[0;32m---> 69\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partitioning_logits\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     71\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     72\u001b[0m     )  \u001b[38;5;66;03m# logits不可使用detach，否则会导致梯度丢失\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:1461\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1461\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1476\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py:258\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1803\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1801\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1803\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1806\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[12], line 152\u001b[0m, in \u001b[0;36mCustomBertForMaskedLM.modify_ffn_activation.<locals>.hook_fn\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_activations\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Ensure the shape matches\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m batch_idx, seq_idx \u001b[38;5;241m=\u001b[39m target_position\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_activation\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m output[batch_idx, seq_idx]\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[batch_idx,\u001b[38;5;250m \u001b[39mseq_idx]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_activation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomBertForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self._intermediate_activations = []\n",
    "        self._original_activations = []\n",
    "        self._mask_logits = None\n",
    "        self._partitioning_activations = []\n",
    "        self._partitioning_step = []\n",
    "        self._partitioning_logits = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        保存现有参数以用于forward_with_partitioning\n",
    "        \"\"\"\n",
    "        self._args = args\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "        # Hook to capture intermediate activations\n",
    "        def hook_fn(module, input, output):\n",
    "            # 如果 _intermediate_activations 不为空，则清空它\n",
    "            if len(self._intermediate_activations) == self.config.num_hidden_layers:\n",
    "                self._intermediate_activations.clear()\n",
    "\n",
    "            # 添加新的激活到 _intermediate_activations\n",
    "            self._intermediate_activations.append(output.detach().cpu())\n",
    "\n",
    "        # Register hooks on intermediate layers\n",
    "        hooks = []\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            hooks.append(layer.intermediate.register_forward_hook(hook_fn))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        # Remove hooks after forward pass\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward_with_partitioning(self, target_position=3):\n",
    "        \"\"\"\n",
    "        target_pos token idx\n",
    "        \"\"\"\n",
    "        for layer in self._intermediate_activations:\n",
    "            target_vector = layer[:, target_position, :]\n",
    "            partitioning, step = self.generate_partitioning(target_vector)\n",
    "            self._partitioning_activations.append(partitioning.detach().cpu())\n",
    "            self._partitioning_step.append(step.detach().cpu())\n",
    "\n",
    "        # Forward pass\n",
    "        for idx in range(self.config.num_hidden_layers):\n",
    "            self.modify_ffn_activation(\n",
    "                idx, target_position, self._partitioning_activations[idx]\n",
    "            )\n",
    "            outputs = super().forward(*self._args, **self._kwargs)\n",
    "            self._partitioning_logits.append(\n",
    "                outputs.logits\n",
    "            )  # logits不可使用detach，否则会导致梯度丢失\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_mask_logits(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Returns logits at [MASK] positions for the given input.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure model is in eval mode\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Find [MASK] token positions (token_id=103 for BERT by default)\n",
    "        mask_token_id = 103\n",
    "        # mask_token_id = self.config.mask_token_id\n",
    "        mask_positions = input_ids == mask_token_id\n",
    "\n",
    "        # Extract logits for [MASK] positions\n",
    "        mask_logits = logits[mask_positions]\n",
    "\n",
    "        return mask_logits\n",
    "\n",
    "    @property\n",
    "    def get_intermediate_activations(self):\n",
    "        \"\"\"\n",
    "        Returns the intermediate neuron activations from the last forward pass.\n",
    "        Raises an error if intermediate_activations is empty.\n",
    "        \"\"\"\n",
    "        if not self._intermediate_activations:\n",
    "            raise ValueError(\n",
    "                \"Intermediate activations are empty. Ensure that a forward pass has been performed.\"\n",
    "            )\n",
    "        return self._intermediate_activations\n",
    "\n",
    "    @property\n",
    "    def original_activations(self):\n",
    "        \"\"\"\n",
    "        Returns the original (before modification) intermediate neuron activations.\n",
    "        \"\"\"\n",
    "        if not self._original_activations:\n",
    "            raise ValueError(\n",
    "                \"Original activations are empty. Ensure that a forward pass has been performed.\"\n",
    "            )\n",
    "        return self._original_activations\n",
    "\n",
    "    @property\n",
    "    def mask_logits(self):\n",
    "        \"\"\"\n",
    "        Returns the logits at [MASK] positions from the last forward pass.\n",
    "        Raises an error if _mask_logits is None.\n",
    "        \"\"\"\n",
    "        if self._mask_logits is None:\n",
    "            raise ValueError(\n",
    "                \"Mask logits are not available. Ensure that a forward pass has been performed with [MASK] tokens.\"\n",
    "            )\n",
    "        return self._mask_logits\n",
    "\n",
    "    def modify_ffn_activation(self, layer_idx, target_position, new_activation):\n",
    "        \"\"\"\n",
    "        Modifies the hidden activations of a specific FFN layer at a specific position in the model.\n",
    "        Stores the original activations before modifying.\n",
    "\n",
    "        Args:\n",
    "            layer_idx (int): Index of the transformer layer to modify (0-indexed).\n",
    "            target_position (tuple): A tuple specifying the target position (batch_idx, seq_idx).\n",
    "            new_activation (torch.Tensor): The new activation values with shape matching the FFN layer output\n",
    "                                            (e.g., [intermediate_size]).\n",
    "        \"\"\"\n",
    "        if not isinstance(new_activation, torch.Tensor):\n",
    "            raise ValueError(\"new_activation must be a torch.Tensor.\")\n",
    "\n",
    "        def hook_fn(module, input, output):\n",
    "            # Store original activations before modification\n",
    "            self._original_activations.append(output.detach().cpu())\n",
    "\n",
    "            # Ensure the shape matches\n",
    "            batch_idx, seq_idx = target_position\n",
    "            if new_activation.shape != output[batch_idx, seq_idx].shape:\n",
    "                raise ValueError(\n",
    "                    f\"Shape mismatch: Expected {output[batch_idx, seq_idx].shape}, \"\n",
    "                    f\"but got {new_activation.shape}.\"\n",
    "                )\n",
    "\n",
    "            # Modify the activation at the target position\n",
    "            output = output.clone()  # Clone to avoid in-place modification\n",
    "            output[batch_idx, seq_idx] = new_activation\n",
    "            return output\n",
    "\n",
    "        # Register the hook\n",
    "        hook = self.bert.encoder.layer[layer_idx].intermediate.register_forward_hook(\n",
    "            hook_fn\n",
    "        )\n",
    "        \"\"\"\n",
    "        这个函数利用hook修改指定位置上的FFN层的激活值，并存储修改前的激活值。\n",
    "        \"\"\"\n",
    "        return hook  # Return the hook handle for later removal if needed\n",
    "\n",
    "    def generate_partitioning(self, vector, times=20):\n",
    "        \"\"\"\n",
    "        生成一组按比例缩放的输入数据。\n",
    "\n",
    "        参数:\n",
    "            emb (torch.Tensor): 输入张量，形状为 (1, ffn_size)，表示一个基准向量。\n",
    "            batch_size (int): 每个批次中的样本数量。\n",
    "            num_batch (int): 批次的总数量。\n",
    "\n",
    "        返回:\n",
    "            partitioning (torch.Tensor): 形状为 (batch_size * num_batch, ffn_size) 的张量，包含生成的缩放输入。\n",
    "            step (torch.Tensor): 每次增量的大小，形状为 (1, ffn_size)，表示每一步的变化量。\n",
    "        equal partitioning\n",
    "\n",
    "        \"\"\"\n",
    "        vector = vector\n",
    "        # vector = vector.unsqueeze(0)\n",
    "        baseline = torch.zeros_like(vector)  # (1, ffn_size)\n",
    "\n",
    "        # step: 计算每一步的增量，即 emb 和 baseline 之间的差距除以总的样本数。\n",
    "        # 这是为了确保从 baseline 到 emb 的变化是均匀的，按比例分配给每个样本。\n",
    "        step = (vector - baseline) / times  # (1, ffn_size) # 每个分量上的间隔值\n",
    "\n",
    "        # res: 使用 baseline 和 step 生成 scaled input 的列表。\n",
    "        # 通过 torch.add 和 step * i 逐步构建每个样本的值，并将这些值沿着第一个维度 (batch_size * num_batch) 拼接。\n",
    "        partitioning = torch.cat(\n",
    "            [torch.add(baseline, step * i) for i in range(times)], dim=0\n",
    "        )  # (step, ffn_size)\n",
    "\n",
    "        # 返回生成的输入数据和每一步的增量。\n",
    "        return partitioning, step[0]\n",
    "\n",
    "    # @staticmethod\n",
    "    def calulate_integrated_gradients(self, logits, target_label, partitioning, step):\n",
    "\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        o = torch.unbind(prob[:, target_label])\n",
    "        gradient = torch.autograd.grad(o, partitioning).sequeeze(0)\n",
    "        grad_summed = gradient.sum(dim=0)  # (ffn_size)\n",
    "        ig_pred = grad_summed * step  # (ffn_size)\n",
    "        return ig_pred\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming CustomBertForMaskedLM is defined as per the provided code\n",
    "\n",
    "\n",
    "def test_custom_bert_for_masked_lm():\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "    # Instantiate the model and move it to the appropriate device (CPU or GPU)\n",
    "    model = CustomBertForMaskedLM(config).to(device)\n",
    "\n",
    "    # Load pre-trained weights into the custom model\n",
    "    model.bert.load_state_dict(\n",
    "        BertForMaskedLM.from_pretrained(model_name).bert.state_dict()\n",
    "    )\n",
    "\n",
    "    # Tokenize some example input text with a [MASK] token\n",
    "    text = \"The quick brown fox jumps over the [MASK] dog.\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Perform the first forward pass to get original logits (before modification)\n",
    "    original_outputs = model(\n",
    "        input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "    )\n",
    "    original_logits = original_outputs.logits\n",
    "\n",
    "    # Get the original logits for the [MASK] position\n",
    "    mask_token_id = 103\n",
    "    mask_positions = input_ids == mask_token_id\n",
    "    original_mask_logits = original_logits[mask_positions]\n",
    "\n",
    "    # Modify the activations of a specific FFN layer and position\n",
    "    target_position = (\n",
    "        0,\n",
    "        5,\n",
    "    )  # Example: batch_idx = 0, seq_idx = 5 (corresponding to the [MASK] token)\n",
    "    new_activation = torch.randn(model.config.intermediate_size).to(\n",
    "        device\n",
    "    )  # Random tensor of appropriate size\n",
    "\n",
    "    # Modify the activation in the 3rd transformer layer\n",
    "    hook = model.modify_ffn_activation(\n",
    "        layer_idx=2, target_position=target_position, new_activation=new_activation\n",
    "    )\n",
    "\n",
    "    # Perform the second forward pass after modification\n",
    "    modified_outputs = model(\n",
    "        input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "    )\n",
    "    modified_logits = modified_outputs.logits\n",
    "\n",
    "    # Get the modified logits for the [MASK] position\n",
    "    modified_mask_logits = modified_logits[mask_positions]\n",
    "\n",
    "    # Compare the logits before and after the modification\n",
    "    difference = torch.abs(original_mask_logits - modified_mask_logits)\n",
    "    forward_with_partitioning = model.forward_with_partitioning(target_position=5)\n",
    "    # Print the results\n",
    "    print(\"Original logits for [MASK] position:\", original_mask_logits)\n",
    "    print(\"Modified logits for [MASK] position:\", modified_mask_logits)\n",
    "    print(\"Difference between original and modified logits:\", difference)\n",
    "    print(f\"Maximum difference: {difference.max()}\")  # Print the maximum difference\n",
    "    partitioning, step = model.generate_partitioning(new_activation, times=20)\n",
    "    re = model.calulate_integrated_gradients(\n",
    "        original_logits, target_label=0, partitioning=partitioning, step=step\n",
    "    )\n",
    "    # Optionally remove the hook after testing\n",
    "    hook.remove()\n",
    "    from pprint import pprint\n",
    "\n",
    "    pprint(model)\n",
    "\n",
    "\n",
    "test_custom_bert_for_masked_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 打印显存统计信息\n",
    "print(torch.cuda.memory_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 24588.08 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:00<00:00, 27279.22 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:01<00:00, 27091.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "dataset = load_dataset(\"imdb\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "new_class_labels = ClassLabel(\n",
    "    num_classes=6, names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\", \"neg\", \"pos\"]\n",
    ")\n",
    "dataset = dataset.cast_column(\"label\", new_class_labels)\n",
    "def update_label(example):\n",
    "        original_label = example[\"label\"]\n",
    "\n",
    "        # 重新映射原始标签d\n",
    "        if original_label == 0:\n",
    "            example[\"label\"] = 4  # 原标签 0 -> 新标签 3\n",
    "        elif original_label == 1:\n",
    "            example[\"label\"] = 5  # 原标签 1 -> 新标签 4\n",
    "\n",
    "        return example\n",
    "\n",
    "dataset = dataset.map(update_label)\n",
    "print(dataset[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random data saved to random_data_10.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Function to generate random data\n",
    "def generate_random_data(num_entries):\n",
    "    data = [[random.randint(0, 11), random.randint(0, 3071)] for _ in range(num_entries)]\n",
    "    return data\n",
    "\n",
    "# Generate data with a specified number of entries\n",
    "num_entries = 10\n",
    "random_data = generate_random_data(num_entries)\n",
    "\n",
    "# Save data to a JSON file\n",
    "output_file = f'random_data_{num_entries}.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(random_data, f, indent=4)\n",
    "\n",
    "print(f\"Random data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b5c8475ceb44809fa785282eef4312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_CNN_Article.csv:  79%|#######8  | 147M/186M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c807d2290d149aa817115766864b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_CNN_Article.csv:   0%|          | 0.00/33.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c7d80000c04dba82d803b761493b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b03526eec38429a8ef2c80c4ad5d165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"contemmcm/ag_news\", \"top4-balanced\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"AyoubChLin/CNN_News_Articles_2011-2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 120000/120000 [00:00<00:00, 236576.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 90000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5700\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 90000/90000 [00:00<00:00, 229994.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5700/5700 [00:00<00:00, 220520.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"fancyzhx/ag_news\", cache_dir=\"/cache/huggingface/datasets\"\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "# target_labels = [\"World\", \"Sports\", \"Business\"]  # 例：指定要保留的标签\n",
    "target_labels = [0, 1, 2]  # 例：指定要保留的标签\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] in target_labels)\n",
    "print(dataset)\n",
    "dataset.save_to_disk(\"/cache/huggingface/datasets/agnews_3labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca69723df235496e8b067ec98b3e584d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f51c0f324c489fa79ef307bad0b8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:  19%|#8        | 10.5M/55.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf-mirror.com/datasets/trivia_qa/455223e4a3ba6977611914d96b1b812fe19a6cd4ea3aef4df381f093b297670c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1739188611&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTE4ODYxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy90cml2aWFfcWEvNDU1MjIzZTRhM2JhNjk3NzYxMTkxNGQ5NmIxYjgxMmZlMTlhNmNkNGVhM2FlZjRkZjM4MWYwOTNiMjk3NjcwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=M14YbTG8tIQQLENI-7PnnJkwTBEmChPlWQyFU4nENzMm3LFSxvo%7EQb8HpF5H87oMCygoxG-6a71NjBqLhV%7EzeYK%7EJn-CN%7EtxPMH%7EanzMqYNchYwwWrkulgMKlf4OGcktXi6EKgVFNiMJQ5usAOAyGkVVlVFxxQQU%7EzBYXPqEC2hbBr5CM46BMpYmqxqXQRTd0ThjCXo6B1yPdntqAfjA4VUZ5jWhXf1yDwIMh4OkwztxOpTX6rQXyBTfz8426s8vYVEvNMUzPnOT7bU5hivU66e1tDDR0MObdjWxXgCZ78o2uPCnvKj1IYnPDEv3HCFCT0wJRHyVoa3KLweLhvSNxw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8814f2e16fd24db49df345f4c93680b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:  19%|#8        | 10.5M/55.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf-mirror.com/datasets/trivia_qa/455223e4a3ba6977611914d96b1b812fe19a6cd4ea3aef4df381f093b297670c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1739188611&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTE4ODYxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy90cml2aWFfcWEvNDU1MjIzZTRhM2JhNjk3NzYxMTkxNGQ5NmIxYjgxMmZlMTlhNmNkNGVhM2FlZjRkZjM4MWYwOTNiMjk3NjcwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=M14YbTG8tIQQLENI-7PnnJkwTBEmChPlWQyFU4nENzMm3LFSxvo%7EQb8HpF5H87oMCygoxG-6a71NjBqLhV%7EzeYK%7EJn-CN%7EtxPMH%7EanzMqYNchYwwWrkulgMKlf4OGcktXi6EKgVFNiMJQ5usAOAyGkVVlVFxxQQU%7EzBYXPqEC2hbBr5CM46BMpYmqxqXQRTd0ThjCXo6B1yPdntqAfjA4VUZ5jWhXf1yDwIMh4OkwztxOpTX6rQXyBTfz8426s8vYVEvNMUzPnOT7bU5hivU66e1tDDR0MObdjWxXgCZ78o2uPCnvKj1IYnPDEv3HCFCT0wJRHyVoa3KLweLhvSNxw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e012effe71f4bf4b384b02927590447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:  19%|#8        | 10.5M/55.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e408e57bbe41f9b3237e245902a408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/7.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36814f143b04908a5ebcd791fcaae7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750e790eb58407eb0f883bdf3f12889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/138384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930711674ba643e8aa2010527e060658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/17944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c542e443c7ad4fbf858b265ab60b5aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/17210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.nocontext\", cache_dir=\"/cache/huggingface/datasets\")\n",
    "\n",
    "# ds = load_dataset(\"lightblue/reranker_continuous_filt_max7_train\", cache_dir=\"/cache/huggingface/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 33497\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 22873/22873 [00:00<00:00, 44621.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2868/2868 [00:00<00:00, 45658.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2786/2786 [00:00<00:00, 50791.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'When cancer spreads to another organ, it most commonly moves to the liver, and now researchers at the Abramson Cancer Center of the University of Pennsylvania say they know why. A new study, published today in Nature, shows hepatocytes—the chief functional cells of the liver—are at the center of a chain reaction that makes it particularly susceptible to cancer cells. These hepatocytes respond to inflammation by activating a protein called STAT3, which in turn increases their production of other proteins called SAA, which then remodel the liver and create the \"soil\" needed for cancer cells to \"seed.\" The researchers show that stopping this process by using antibodies that block IL-6—the inflammatory signal that drives this chain reaction—can limit the potential of cancer to spread to the liver. \"The seed-and-soil hypothesis is well-recognized, but our research now shows that hepatocytes are the major orchestrators of this process,\" said senior author Gregory L. Beatty, MD, Ph.D., an assistant professor of Hematology-Oncology at Penn\\'s Perelman School of Medicine. Jae W. Lee, an MD/Ph.D. candidate in Beatty\\'s laboratory, is the lead author. For this study, the team first used mouse models of pancreatic ductal adenocarcinoma (PDAC), the most common type of pancreatic cancer and currently the third leading cause of cancer death in the United States. They found that nearly all hepatocytes showed STAT3 activation in mice with cancer, compared to less than two percent of hepatocytes in mice without tumors. They then partnered with investigators at the Mayo Clinic Arizona and other Penn colleagues to show that this same biology could be seen in patients with pancreatic cancer as well colon and lung cancer. Genetically deleting STAT3 only in hepatocytes effectively blocked the increased susceptibility of the liver to cancer seeding in mice. The team collaborated further with investigators at the University of Kentucky to show that IL-6 controls STAT3 signaling in these cells and instructs hepatocytes to make SAA, which acts as an alarm to attract inflammatory cells and initiate a fibrotic reaction that together establish the \"soil.\" \"The liver is an important sensor in the body,\" Lee said. \"We show that hepatocytes sense inflammation and respond in a structured way that cancer uses to help it spread.\" The study also found that IL-6 drives changes in the liver whether there\\'s a tumor present or not, implying that any condition associated with increased IL-6 levels—such as obesity or cardiovascular disease, among others—could affect the liver\\'s receptiveness to cancer. Researchers say this provides evidence that therapies which target hepatocytes may be able to prevent cancer from spreading to the liver, a major cause of cancer mortality. ', 'label': 'Medicine'}\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 33497\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "ds = load_dataset(\"dongqi-me/SciNews\")\n",
    "target_labels = [\"Physics\", \"Medicine\", \"Biology\"]  # 例：指定要保留的标签\n",
    "ds = ds.select_columns([\"News_Body\", \"Topic\"])\n",
    "ds = ds.rename_columns({\"News_Body\": \"text\", \"Topic\": \"label\"} )\n",
    "print(ds[\"train\"])\n",
    "\n",
    "# 过滤数据集，保留 'Topic' 中属于 target_labels 的行\n",
    "filtered_ds = ds.filter(lambda x: x[\"label\"] in target_labels)\n",
    "filtered_ds.save_to_disk(\"/cache/huggingface/datasets/SciNews_3labels\")\n",
    "\n",
    "# 输出结果\n",
    "print(filtered_ds[\"train\"][0])\n",
    "print(ds[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be99bbd9f44fcd9114717839e1d5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=40):   0%|          | 0/33497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ba82af710d4aa98fbc3636b8dbc9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=40):   0%|          | 0/4187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05c04f52be34e728e370678616e23db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=40):   0%|          | 0/4188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer', 'Nano', 'Space', 'Physics', 'Chemistry', 'Medicine', 'Biology', 'Earth']\n",
      "{'text': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)}\n",
      "Value(dtype='string', id=None)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "ds = load_dataset(\"dongqi-me/SciNews\")\n",
    "ds = ds.select_columns([\"News_Body\", \"Topic\"])\n",
    "ds = ds.rename_columns({\"News_Body\": \"text\", \"Topic\": \"label\"} )\n",
    "target_labels = [\"Other\"]  # 例：指定要保留的标签\n",
    "ds = ds.filter(lambda x: x[\"label\"] not in target_labels, num_proc=40)\n",
    "\n",
    "print(label_names := list(set(ds[\"train\"][\"label\"])))\n",
    "print(ds[\"train\"].features)\n",
    "print(ds[\"train\"].features['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1244103\n",
      "})\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Business', 'Entertainment', 'Europe', 'Health', 'Italia', 'Music Feeds', 'Sci/Tech', 'Software and Developement', 'Sports', 'Toons', 'Top News', 'Top Stories', 'U.S.', 'World'], id=None)}\n",
      "['Business', 'Entertainment', 'Europe', 'Health', 'Italia', 'Music Feeds', 'Sci/Tech', 'Software and Developement', 'Sports', 'Toons', 'Top News', 'Top Stories', 'U.S.', 'World']\n",
      "['Business', 'Entertainment', 'Europe', 'Health', 'Italia', 'Music Feeds', 'Sci/Tech', 'Software and Developement', 'Sports', 'Toons', 'Top News', 'Top Stories', 'U.S.', 'World', 'Computer', 'Nano', 'Space', 'Physics', 'Chemistry', 'Medicine', 'Biology', 'Earth']\n",
      "22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606133423bf24055bb505a5740e96d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/942111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549b37d2e7504d35b0b8a5eb61ba3733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/104680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 104680\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "import datasets\n",
    "ds = load_dataset(\"contemmcm/ag_news\", \"original\")[\"complete\"]\n",
    "ds = ds.select_columns([\"text\", \"label\"])\n",
    "\n",
    "print(ds)\n",
    "print(ds.features)\n",
    "# target_labels = [\"Physics\", \"Medicine\", \"Biology\"]  # 例：指定要保留的标签\n",
    "target_labels = [\"Sci/Tech\", \"Health\"]  # 例：指定要保留的标签\n",
    "# target_labels = [1, 0]  # 例：指定要保留的标签\n",
    "label_names = ds.features['label'].names\n",
    "print(label_names)\n",
    "label_indices_to_remove = []\n",
    "for label in target_labels:\n",
    "    label_indices_to_remove.append(label_names.index(label))\n",
    "ds = ds.filter(lambda x: x[\"label\"] not in label_indices_to_remove, num_proc=40)\n",
    "# print(ds[\"train\"])\n",
    "# filtered_label_names = [name for i, name in enumerate(label_names) if i not in label_indices_to_remove]\n",
    "filtered_label_names = [\n",
    "    \"Business\", \"Entertainment\", \"Europe\", \"Health\", \"Italia\", \"Music Feeds\", \"Sci/Tech\", \n",
    "    \"Software and Developement\", \"Sports\", \"Toons\", \"Top News\", \"Top Stories\", \n",
    "    \"U.S.\", \"World\", \"Computer\", \"Nano\", \"Space\", \"Physics\", \"Chemistry\", \n",
    "    \"Medicine\", \"Biology\", \"Earth\"\n",
    "]\n",
    "\n",
    "# print(filtered_label_names)\n",
    "ds = ds.cast_column(\"label\", datasets.ClassLabel(names=filtered_label_names))\n",
    "print(ds.features['label'].names)\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "\n",
    "num_labels = len(ds[\"train\"].features[\"label\"].names)\n",
    "print(num_labels)\n",
    "\n",
    "\n",
    "ds.save_to_disk(f\"/cache/huggingface/datasets/ag_news_14labels\")\n",
    "\n",
    "print(ds[\"test\"])\n",
    "\n",
    "# 过滤数据集，保留 'Topic' 中属于 target_labels 的行\n",
    "\n",
    "\n",
    "# filtered_ds.save_to_disk(\"/cache/huggingface/datasets/SciNews_3labels\")\n",
    "\n",
    "# # 输出结果\n",
    "# print(filtered_ds[\"train\"][0])\n",
    "# print(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2150cbed3084373b1e79adf4719ddf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=30):   0%|          | 0/942111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (802 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1433 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1261 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py:704\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/managers.py:821\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    820\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 821\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/connection.py:253\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 253\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/connection.py:433\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 433\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/connection.py:398\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 应用过滤函数\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cache/huggingface/datasets/ag_news_14labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_by_token_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m ds\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cache/huggingface/datasets/ag_news_14labels_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(ds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/dataset_dict.py:1001\u001b[0m, in \u001b[0;36mDatasetDict.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m   1000\u001b[0m     {\n\u001b[0;32m-> 1001\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1017\u001b[0m     }\n\u001b[1;32m   1018\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py:3688\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3688\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3710\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3715\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3716\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py:3165\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3161\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3162\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3163\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py:770\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_success:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "max_token_length = 512\n",
    "\n",
    "# 定义基于 token 长度的过滤函数\n",
    "def filter_by_token_length(example):\n",
    "    # 使用 tokenizer 对句子进行编码，得到 token 数量\n",
    "    encoding = tokenizer(example['text'], padding=False)\n",
    "    \n",
    "    # 判断 token 数量是否超过最大限制\n",
    "    return len(encoding['input_ids']) <= max_token_length \n",
    "\n",
    "# 应用过滤函数\n",
    "\n",
    "ds = load_from_disk(\"/cache/huggingface/datasets/ag_news_14labels\")\n",
    "ds = ds.filter(filter_by_token_length,num_proc=30)\n",
    "ds.save_to_disk(f\"/cache/huggingface/datasets/ag_news_14labels_test\")\n",
    "\n",
    "print(ds)\n",
    "# print(ds[\"test\"][0])\n",
    "# print(len(ds[\"test\"][0]['text']))\n",
    "# text = ds[\"test\"][0]['text']\n",
    "# print(tokenizer(text))\n",
    "# print(len(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"/root/ftg/results/new_6tags_agnews_based_imdb\")\n",
    "peft_model_id = \"/root/ftg/results/imdb_01_17_10:18/checkpoint-4689\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"model_id\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "Question: Who predicted Hawking radiation?\n",
      "Answer: stephen hawking\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 示例上下文和问题\n",
    "context = \"\"\"Hawking radiation is theoretical radiation that is predicted to be emitted by black holes. It is named after physicist Stephen Hawking, who predicted it in 1974. This radiation is a result of quantum mechanical effects near the event horizon of a black hole.\"\"\"\n",
    "question = \"Who predicted Hawking radiation?\"\n",
    "\n",
    "# 分词并生成模型输入\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 获取答案的起始和结束位置\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits)\n",
    "\n",
    "# 解码出答案\n",
    "answer_tokens = inputs.input_ids[0][start_idx:end_idx+1]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(model)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! A large language model is an artificial intelligence (AI) system that can generate human-like text on its own. These models are designed to mimic the complexity and creativity of natural language, allowing them to understand and respond to a wide range of prompts and inputs.\n",
      "\n",
      "Large language models are commonly used in various applications such as chatbots, virtual assistants, machine translation, and text generation for tasks like writing essays, creating content, or even generating creative writing. They are also employed in areas like language translation, sentiment analysis, and more complex NLP tasks. The use of large language models has grown significantly over the years due to their ability to process vast amounts of data quickly and accurately, making them indispensable tools in today's digital age.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33b578d11e84addbefde374179394f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d87be20339446d3987984a632d92708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b44c8477584edca3095ef90f22f832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bee53be5234bfaa40a2f1147ce5c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3973c9fcfa52405ba582c8c5203e17e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3f5b091a37477a8abecfc445ed1def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8004dd8575e4f92a8bef9c1d7b85b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.479400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# MODEL_NAME = \"unsloth/Llama-3.2-1B\" # Try Llama if you want\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载你的数据集，假设数据集包含 'text' 列作为输入文本\n",
    "data = load_dataset(\"HuggingFaceH4/helpful-instructions\")\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    full_prompt = \"\"\n",
    "    for sentence in data_point:\n",
    "        full_prompt += (\n",
    "            \"<human>: \" + sentence[0] + \"  \\n <assistant>: \" + sentence[1] + \"\\n\"\n",
    "        )\n",
    "    return full_prompt  # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: response\" (DONE)\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"experiments\"\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(prepare_model_for_kbit_training(model), config)\n",
    "data = data[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=200,  # try more steps if you can\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存训练后的模型\n",
    "trainer.save_model(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is on: cuda:0\n",
      "z is on: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个在 GPU 上的 tensor，并启用梯度计算\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True).cuda()\n",
    "\n",
    "# 对 x 进行一些操作\n",
    "y = x * 2\n",
    "\n",
    "# 执行 .detach().cpu()\n",
    "z = y.detach().cpu()\n",
    "\n",
    "# 打印 y 和 z 的设备信息\n",
    "print(\"y is on:\", y.device)  # y 仍然在 GPU 上\n",
    "print(\"z is on:\", z.device)  # z 已被移动到 CPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([100.,   4.,   6.], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "z: tensor([100.,   4.,   6.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个在 GPU 上的 tensor，并启用梯度计算\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True).cuda()\n",
    "\n",
    "# 对 x 进行一些操作\n",
    "y = x * 2\n",
    "\n",
    "# 执行 .detach().cpu()\n",
    "z = y.detach().cpu()\n",
    "\n",
    "# 修改 z\n",
    "z[0] = 100.0\n",
    "\n",
    "# 打印 y 和 z\n",
    "print(\"y:\", y)  # y 仍然在 GPU 上，未被修改\n",
    "print(\"z:\", z)  # z 已被修改"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
