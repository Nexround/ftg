{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "获取模型参数量"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=\"/cache/huggingface/hub\")\n",
    "\n",
    "# 计算总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"模型的总参数量: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比较模型参数差异并提取位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def compare_mlp_params(model1, model2, mlp_layers: Union[str, List[str]]) -> dict:\n",
    "    \"\"\"比较多个MLP层的参数差异\n",
    "\n",
    "    Args:\n",
    "        mlp_layers: 支持以下格式：\n",
    "            - 单个层模式: \"transformer.h.0.mlp\"\n",
    "            - 多个层模式: [\"transformer.h.0.mlp\", \"transformer.h.5.mlp\"]\n",
    "    \"\"\"\n",
    "    # 统一处理为列表格式\n",
    "    if isinstance(mlp_layers, str):\n",
    "        target_patterns = [mlp_layers]\n",
    "    else:\n",
    "        target_patterns = mlp_layers\n",
    "\n",
    "    # 多模式参数提取\n",
    "    def filter_params(model):\n",
    "        return {\n",
    "            name: param\n",
    "            for name, param in model.named_parameters()\n",
    "            if any(pattern in name for pattern in target_patterns)\n",
    "        }\n",
    "\n",
    "    params1 = filter_params(model1)\n",
    "    params2 = filter_params(model2)\n",
    "\n",
    "    # 结构一致性检查\n",
    "    if params1.keys() != params2.keys():\n",
    "        missing_in_1 = set(params2.keys()) - set(params1.keys())\n",
    "        missing_in_2 = set(params1.keys()) - set(params2.keys())\n",
    "        raise ValueError(\n",
    "            f\"模型结构不一致\\n\"\n",
    "            f\"Model1缺失层: {list(missing_in_1)}\\n\"\n",
    "            f\"Model2缺失层: {list(missing_in_2)}\"\n",
    "        )\n",
    "\n",
    "    differences = {}\n",
    "\n",
    "    for name in params1:\n",
    "        p1, p2 = params1[name].cpu(), params2[name].cpu()\n",
    "\n",
    "        if p1.shape != p2.shape:\n",
    "            raise ValueError(f\"形状不匹配: {name} | {p1.shape} vs {p2.shape}\")\n",
    "\n",
    "        if not torch.equal(p1, p2):\n",
    "            diff_mask = ~torch.isclose(p1, p2, rtol=1e-5, atol=1e-8)\n",
    "            diff_indices = torch.unique(torch.nonzero(diff_mask)[:, 0])\n",
    "\n",
    "            differences[name] = {\n",
    "                \"shape\": tuple(p1.shape),\n",
    "                \"diff_count\": diff_indices.size(0),\n",
    "                \"diff_ratio\": diff_indices.size(0) / p1.numel(),\n",
    "                \"diff_indices\": diff_indices.tolist(),\n",
    "            }\n",
    "\n",
    "    return differences\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 单层比较\n",
    "    # diff = compare_mlp_params(model1_path, model2_path, \"transformer.h.0.mlp\")\n",
    "    model1_path = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    model2_path = \"/cache/models/loki_reranker_qwen2_5-0-5b-40_real\"\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        model1_path, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(\n",
    "        model2_path, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    # for name, param in model1.named_parameters():\n",
    "    #     print(name)\n",
    "    target_modules = []\n",
    "    for idx, layer in enumerate(model1.model.layers):\n",
    "        module_str = f\"model.layers.{idx}.mlp.down_proj.weight\"\n",
    "        target_modules.append(module_str)\n",
    "    # 多层比较\n",
    "    diff = compare_mlp_params(model1, model2, mlp_layers=target_modules)\n",
    "    pprint(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打印模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"/cache/models/loki_reranker_qwen2_5-0-5b-5_real\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "with safe_open(\"/cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688/model.safetensors\", framework=\"pt\") as f:\n",
    "    print(f.keys())  # 直接输出所有权重键名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 还原模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers import AutoModelForCausalLM\n",
    "from src.module.loki_linear import LoKILinear\n",
    "import json\n",
    "import torch\n",
    "\n",
    "checkpoint_path = (\n",
    "    \"/cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688/model.safetensors\"\n",
    ")\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/5.json\"\n",
    "target_model = \"/cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "trainable_neurons = list(data)\n",
    "# 重新初始化原始模型结构\n",
    "original_model = AutoModelForCausalLM.from_pretrained(target_model)\n",
    "\n",
    "\n",
    "def merge_loki_weights(loki_layer, original_linear):\n",
    "    # 合并权重矩阵\n",
    "    merged_weight = torch.zeros_like(original_linear.weight.data)\n",
    "    merged_weight[loki_layer.active_pos] = loki_layer.active_part.weight.data\n",
    "    merged_weight[loki_layer.fixed_pos] = loki_layer.fixed_part.weight.data\n",
    "\n",
    "    # 合并偏置项\n",
    "    if original_linear.bias is not None:\n",
    "        merged_bias = torch.zeros_like(original_linear.bias.data)\n",
    "        merged_bias[loki_layer.active_pos] = loki_layer.active_bias.data\n",
    "        merged_bias[loki_layer.fixed_pos] = loki_layer.fixed_bias.data\n",
    "\n",
    "    # 加载参数到原始层\n",
    "    original_linear.weight.data.copy_(merged_weight)\n",
    "    if original_linear.bias is not None:\n",
    "        original_linear.bias.data.copy_(merged_bias)\n",
    "\n",
    "\n",
    "# 加载检查点文件\n",
    "\n",
    "# 遍历所有层还原参数\n",
    "for layer_idx in range(original_model.config.num_hidden_layers):\n",
    "    # 获取当前层的原始结构\n",
    "    original_down_proj = original_model.model.layers[layer_idx].mlp.down_proj\n",
    "\n",
    "    # 加载LoKI层参数\n",
    "    with safe_open(checkpoint_path, framework=\"pt\") as f:\n",
    "        # 创建临时LoKI层用于加载参数\n",
    "        loki_layer = LoKILinear(\n",
    "            original_down_proj, target_neurons=trainable_neurons[layer_idx]\n",
    "        )\n",
    "        loki_layer.load_state_dict(\n",
    "            {\n",
    "                \"active_part.weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.active_part.weight\"\n",
    "                ),\n",
    "                \"fixed_part.weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.fixed_part.weight\"\n",
    "                ),\n",
    "                # \"active_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.active_bias\"\n",
    "                # ),\n",
    "                # \"fixed_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.fixed_bias\"\n",
    "                # ),\n",
    "            },\n",
    "            strict=True,\n",
    "        )\n",
    "        weight = f.get_tensor(f\"model.layers.{layer_idx}.mlp.down_proj.active_part.weight\")\n",
    "\n",
    "    # 合并参数到原始层\n",
    "    merge_loki_weights(loki_layer, original_down_proj)\n",
    "\n",
    "# 保存还原后的模型\n",
    "original_model.save_pretrained(\"/cache/models/loki_reranker_qwen2_5-0-5b-5_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers import AutoModelForCausalLM\n",
    "from src.loki.loki_linear import LoKILinear\n",
    "import json\n",
    "import torch\n",
    "\n",
    "checkpoint_path = (\n",
    "    \"/cache/models/loki_reranker_qwen2_5-0-5b-40/model.safetensors\"\n",
    ")\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/40.json\"\n",
    "target_model = \"/cache/models/loki_reranker_qwen2_5-0-5b-40\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "trainable_neurons = list(data)\n",
    "# 重新初始化原始模型结构\n",
    "original_model = AutoModelForCausalLM.from_pretrained(target_model)\n",
    "\n",
    "\n",
    "def merge_loki_weights(loki_layer, original_linear):\n",
    "    # 合并权重矩阵\n",
    "    merged_weight = torch.zeros_like(original_linear.weight.data)\n",
    "    merged_weight[loki_layer.active_pos] = loki_layer.active_weight.data\n",
    "    merged_weight[loki_layer.fixed_pos] = loki_layer.fixed_weight.data\n",
    "\n",
    "    # 合并偏置项\n",
    "    if original_linear.bias is not None:\n",
    "        merged_bias = torch.zeros_like(original_linear.bias.data)\n",
    "        merged_bias[loki_layer.active_pos] = loki_layer.active_bias.data\n",
    "        merged_bias[loki_layer.fixed_pos] = loki_layer.fixed_bias.data\n",
    "\n",
    "    # 加载参数到原始层\n",
    "    original_linear.weight.data.copy_(merged_weight)\n",
    "    if original_linear.bias is not None:\n",
    "        original_linear.bias.data.copy_(merged_bias)\n",
    "\n",
    "\n",
    "# 加载检查点文件\n",
    "\n",
    "# 遍历所有层还原参数\n",
    "for layer_idx in range(original_model.config.num_hidden_layers):\n",
    "    # 获取当前层的原始结构\n",
    "    original_down_proj = original_model.model.layers[layer_idx].mlp.down_proj\n",
    "\n",
    "    # 加载LoKI层参数\n",
    "    with safe_open(checkpoint_path, framework=\"pt\") as f:\n",
    "        # 创建临时LoKI层用于加载参数\n",
    "        loki_layer = LoKILinear(\n",
    "            original_down_proj, target_neurons=trainable_neurons[layer_idx]\n",
    "        )\n",
    "        loki_layer.load_state_dict(\n",
    "            {\n",
    "                \"active_weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.active_weight\"\n",
    "                ),\n",
    "                \"fixed_weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.fixed_weight\"\n",
    "                ),\n",
    "                \"index_map\" : loki_layer.index_map\n",
    "                # \"active_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.active_bias\"\n",
    "                # ),\n",
    "                # \"fixed_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.fixed_bias\"\n",
    "                # ),\n",
    "            },\n",
    "            strict=True,\n",
    "        )\n",
    "        weight = f.get_tensor(f\"model.layers.{layer_idx}.mlp.down_proj.active_weight\")\n",
    "\n",
    "    # 合并参数到原始层\n",
    "    merge_loki_weights(loki_layer, original_down_proj)\n",
    "\n",
    "# 保存还原后的模型\n",
    "original_model.save_pretrained(\"/cache/models/loki_reranker_qwen2_5-0-5b-40_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.qwen_loki import LoKIQwen2ForCausalLM\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/5.json\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# custom_config = LoKIQwen2Config(\"Qwen/Qwen2.5-0.5B-Instruct\", target_neurons=data)\n",
    "model = LoKIQwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    target_neurons=data,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(model)\n",
    "# 验证第一层权重是否迁移\n",
    "loki_layer = model.model.layers[0].mlp.down_proj\n",
    "\n",
    "pretrained_weight = original_model.model.layers[0].mlp.down_proj.weight\n",
    "\n",
    "combined_weight = torch.zeros_like(pretrained_weight)\n",
    "combined_weight[loki_layer.active_pos] = loki_layer.active_part.weight.data\n",
    "combined_weight[loki_layer.fixed_pos] = loki_layer.fixed_part.weight.data\n",
    "print(torch.allclose(combined_weight, pretrained_weight, atol=1e-6))  # 应输出 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loki.qwen_loki import LoKIQwen2ForCausalLM\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, Qwen2Config\n",
    "\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/5.json\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# custom_config = LoKIQwen2Config(\"Qwen/Qwen2.5-0.5B-Instruct\", target_neurons=data)\n",
    "model = LoKIQwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    target_neurons=data,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "LoKIQwen2ForCausalLM.register_for_auto_class(\"AutoModelForCausalLM\")\n",
    "\n",
    "# AutoModel.register(Qwen2Config, LoKIQwen2ForCausalLM)\n",
    "# LoKIQwen2ForCausalLM.register_for_auto_class(\"AutoModel\")\n",
    "model.save_pretrained(\"/cache/models/custom-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义模型加载测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/10.json\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/cache/models/Qwen2.5-3B-Instruct\",\n",
    "    target_neurons=data,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/cache/models/custom-model-test\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载后Model还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loki.loki_linear import restore_original_linears\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/cache/models/custom-model-test\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)\n",
    "model = restore_original_linears(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loki.tools import restore_loki_model\n",
    "restore_loki_model(\n",
    "    model_path=\"/cache/models/loki_reranker_qwen2_5-0-5b-10_invert\",\n",
    "    target_neurons_path=\"target_neurons/Qwen2.5-0.5B-Instruct/inverted/10.json\",\n",
    "    output_path=\"/cache/models/loki_reranker_qwen2_5-0-5b-10_invert_real\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loki.tools import create_and_save_loki_model\n",
    "\n",
    "create_and_save_loki_model(\n",
    "    target_neurons_path=\"/workspace/ftg/target_neurons/Qwen2.5-0.5B-Instruct/5.json\",\n",
    "    save_dir=\"/cache/models/custom-model-test\",\n",
    "    model_name=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
