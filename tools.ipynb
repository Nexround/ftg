{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "获取模型参数量"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=\"/cache/huggingface/hub\")\n",
    "\n",
    "# 计算总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"模型的总参数量: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比较模型参数差异并提取位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.layers.0.mlp.down_proj.weight': {'diff_count': 6,\n",
      "                                         'diff_indices': [424,\n",
      "                                                          483,\n",
      "                                                          541,\n",
      "                                                          757,\n",
      "                                                          864,\n",
      "                                                          878],\n",
      "                                         'diff_ratio': 1.3767328477443608e-06,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.1.mlp.down_proj.weight': {'diff_count': 13,\n",
      "                                         'diff_indices': [19,\n",
      "                                                          38,\n",
      "                                                          62,\n",
      "                                                          96,\n",
      "                                                          143,\n",
      "                                                          208,\n",
      "                                                          245,\n",
      "                                                          262,\n",
      "                                                          272,\n",
      "                                                          435,\n",
      "                                                          443,\n",
      "                                                          520,\n",
      "                                                          754],\n",
      "                                         'diff_ratio': 2.982921170112782e-06,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.10.mlp.down_proj.weight': {'diff_count': 75,\n",
      "                                          'diff_indices': [5,\n",
      "                                                           10,\n",
      "                                                           15,\n",
      "                                                           17,\n",
      "                                                           18,\n",
      "                                                           23,\n",
      "                                                           62,\n",
      "                                                           78,\n",
      "                                                           101,\n",
      "                                                           125,\n",
      "                                                           134,\n",
      "                                                           139,\n",
      "                                                           140,\n",
      "                                                           149,\n",
      "                                                           165,\n",
      "                                                           177,\n",
      "                                                           187,\n",
      "                                                           192,\n",
      "                                                           198,\n",
      "                                                           202,\n",
      "                                                           237,\n",
      "                                                           242,\n",
      "                                                           260,\n",
      "                                                           262,\n",
      "                                                           277,\n",
      "                                                           280,\n",
      "                                                           283,\n",
      "                                                           289,\n",
      "                                                           292,\n",
      "                                                           308,\n",
      "                                                           321,\n",
      "                                                           326,\n",
      "                                                           331,\n",
      "                                                           337,\n",
      "                                                           339,\n",
      "                                                           341,\n",
      "                                                           371,\n",
      "                                                           397,\n",
      "                                                           404,\n",
      "                                                           409,\n",
      "                                                           451,\n",
      "                                                           474,\n",
      "                                                           479,\n",
      "                                                           483,\n",
      "                                                           500,\n",
      "                                                           501,\n",
      "                                                           513,\n",
      "                                                           525,\n",
      "                                                           528,\n",
      "                                                           534,\n",
      "                                                           550,\n",
      "                                                           556,\n",
      "                                                           559,\n",
      "                                                           596,\n",
      "                                                           635,\n",
      "                                                           672,\n",
      "                                                           722,\n",
      "                                                           737,\n",
      "                                                           742,\n",
      "                                                           744,\n",
      "                                                           751,\n",
      "                                                           760,\n",
      "                                                           763,\n",
      "                                                           772,\n",
      "                                                           778,\n",
      "                                                           782,\n",
      "                                                           793,\n",
      "                                                           820,\n",
      "                                                           823,\n",
      "                                                           852,\n",
      "                                                           855,\n",
      "                                                           869,\n",
      "                                                           883,\n",
      "                                                           886,\n",
      "                                                           895],\n",
      "                                          'diff_ratio': 1.7209160596804512e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.11.mlp.down_proj.weight': {'diff_count': 82,\n",
      "                                          'diff_indices': [1,\n",
      "                                                           15,\n",
      "                                                           24,\n",
      "                                                           52,\n",
      "                                                           60,\n",
      "                                                           77,\n",
      "                                                           96,\n",
      "                                                           116,\n",
      "                                                           119,\n",
      "                                                           130,\n",
      "                                                           135,\n",
      "                                                           143,\n",
      "                                                           147,\n",
      "                                                           149,\n",
      "                                                           166,\n",
      "                                                           167,\n",
      "                                                           191,\n",
      "                                                           199,\n",
      "                                                           209,\n",
      "                                                           222,\n",
      "                                                           223,\n",
      "                                                           231,\n",
      "                                                           238,\n",
      "                                                           241,\n",
      "                                                           254,\n",
      "                                                           260,\n",
      "                                                           261,\n",
      "                                                           262,\n",
      "                                                           265,\n",
      "                                                           307,\n",
      "                                                           316,\n",
      "                                                           328,\n",
      "                                                           345,\n",
      "                                                           348,\n",
      "                                                           354,\n",
      "                                                           368,\n",
      "                                                           374,\n",
      "                                                           381,\n",
      "                                                           397,\n",
      "                                                           430,\n",
      "                                                           433,\n",
      "                                                           447,\n",
      "                                                           460,\n",
      "                                                           470,\n",
      "                                                           473,\n",
      "                                                           488,\n",
      "                                                           490,\n",
      "                                                           492,\n",
      "                                                           496,\n",
      "                                                           506,\n",
      "                                                           515,\n",
      "                                                           521,\n",
      "                                                           529,\n",
      "                                                           543,\n",
      "                                                           559,\n",
      "                                                           606,\n",
      "                                                           632,\n",
      "                                                           637,\n",
      "                                                           639,\n",
      "                                                           669,\n",
      "                                                           682,\n",
      "                                                           718,\n",
      "                                                           719,\n",
      "                                                           734,\n",
      "                                                           740,\n",
      "                                                           744,\n",
      "                                                           745,\n",
      "                                                           747,\n",
      "                                                           750,\n",
      "                                                           767,\n",
      "                                                           778,\n",
      "                                                           784,\n",
      "                                                           801,\n",
      "                                                           811,\n",
      "                                                           838,\n",
      "                                                           846,\n",
      "                                                           848,\n",
      "                                                           852,\n",
      "                                                           859,\n",
      "                                                           865,\n",
      "                                                           875,\n",
      "                                                           889],\n",
      "                                          'diff_ratio': 1.881534891917293e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.12.mlp.down_proj.weight': {'diff_count': 82,\n",
      "                                          'diff_indices': [5,\n",
      "                                                           11,\n",
      "                                                           15,\n",
      "                                                           17,\n",
      "                                                           24,\n",
      "                                                           29,\n",
      "                                                           38,\n",
      "                                                           42,\n",
      "                                                           45,\n",
      "                                                           52,\n",
      "                                                           60,\n",
      "                                                           66,\n",
      "                                                           74,\n",
      "                                                           77,\n",
      "                                                           78,\n",
      "                                                           82,\n",
      "                                                           115,\n",
      "                                                           120,\n",
      "                                                           184,\n",
      "                                                           195,\n",
      "                                                           198,\n",
      "                                                           209,\n",
      "                                                           232,\n",
      "                                                           239,\n",
      "                                                           241,\n",
      "                                                           252,\n",
      "                                                           293,\n",
      "                                                           301,\n",
      "                                                           316,\n",
      "                                                           327,\n",
      "                                                           333,\n",
      "                                                           348,\n",
      "                                                           383,\n",
      "                                                           390,\n",
      "                                                           400,\n",
      "                                                           402,\n",
      "                                                           409,\n",
      "                                                           432,\n",
      "                                                           436,\n",
      "                                                           441,\n",
      "                                                           443,\n",
      "                                                           449,\n",
      "                                                           450,\n",
      "                                                           457,\n",
      "                                                           468,\n",
      "                                                           473,\n",
      "                                                           481,\n",
      "                                                           483,\n",
      "                                                           489,\n",
      "                                                           500,\n",
      "                                                           502,\n",
      "                                                           510,\n",
      "                                                           513,\n",
      "                                                           517,\n",
      "                                                           521,\n",
      "                                                           522,\n",
      "                                                           538,\n",
      "                                                           542,\n",
      "                                                           543,\n",
      "                                                           593,\n",
      "                                                           600,\n",
      "                                                           607,\n",
      "                                                           612,\n",
      "                                                           614,\n",
      "                                                           622,\n",
      "                                                           644,\n",
      "                                                           668,\n",
      "                                                           680,\n",
      "                                                           682,\n",
      "                                                           695,\n",
      "                                                           703,\n",
      "                                                           740,\n",
      "                                                           742,\n",
      "                                                           806,\n",
      "                                                           812,\n",
      "                                                           820,\n",
      "                                                           833,\n",
      "                                                           839,\n",
      "                                                           849,\n",
      "                                                           855,\n",
      "                                                           861,\n",
      "                                                           877],\n",
      "                                          'diff_ratio': 1.881534891917293e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.13.mlp.down_proj.weight': {'diff_count': 75,\n",
      "                                          'diff_indices': [9,\n",
      "                                                           25,\n",
      "                                                           30,\n",
      "                                                           37,\n",
      "                                                           41,\n",
      "                                                           64,\n",
      "                                                           83,\n",
      "                                                           98,\n",
      "                                                           116,\n",
      "                                                           123,\n",
      "                                                           135,\n",
      "                                                           141,\n",
      "                                                           163,\n",
      "                                                           191,\n",
      "                                                           192,\n",
      "                                                           231,\n",
      "                                                           236,\n",
      "                                                           238,\n",
      "                                                           241,\n",
      "                                                           273,\n",
      "                                                           289,\n",
      "                                                           294,\n",
      "                                                           301,\n",
      "                                                           316,\n",
      "                                                           335,\n",
      "                                                           336,\n",
      "                                                           354,\n",
      "                                                           358,\n",
      "                                                           376,\n",
      "                                                           394,\n",
      "                                                           401,\n",
      "                                                           404,\n",
      "                                                           418,\n",
      "                                                           446,\n",
      "                                                           456,\n",
      "                                                           479,\n",
      "                                                           485,\n",
      "                                                           505,\n",
      "                                                           512,\n",
      "                                                           521,\n",
      "                                                           559,\n",
      "                                                           561,\n",
      "                                                           572,\n",
      "                                                           575,\n",
      "                                                           588,\n",
      "                                                           607,\n",
      "                                                           642,\n",
      "                                                           646,\n",
      "                                                           648,\n",
      "                                                           652,\n",
      "                                                           658,\n",
      "                                                           678,\n",
      "                                                           679,\n",
      "                                                           683,\n",
      "                                                           694,\n",
      "                                                           695,\n",
      "                                                           710,\n",
      "                                                           711,\n",
      "                                                           738,\n",
      "                                                           745,\n",
      "                                                           764,\n",
      "                                                           783,\n",
      "                                                           787,\n",
      "                                                           808,\n",
      "                                                           812,\n",
      "                                                           820,\n",
      "                                                           825,\n",
      "                                                           828,\n",
      "                                                           829,\n",
      "                                                           838,\n",
      "                                                           844,\n",
      "                                                           859,\n",
      "                                                           882,\n",
      "                                                           888,\n",
      "                                                           892],\n",
      "                                          'diff_ratio': 1.7209160596804512e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.14.mlp.down_proj.weight': {'diff_count': 68,\n",
      "                                          'diff_indices': [11,\n",
      "                                                           17,\n",
      "                                                           60,\n",
      "                                                           64,\n",
      "                                                           110,\n",
      "                                                           113,\n",
      "                                                           122,\n",
      "                                                           133,\n",
      "                                                           164,\n",
      "                                                           183,\n",
      "                                                           186,\n",
      "                                                           207,\n",
      "                                                           245,\n",
      "                                                           286,\n",
      "                                                           318,\n",
      "                                                           354,\n",
      "                                                           356,\n",
      "                                                           364,\n",
      "                                                           405,\n",
      "                                                           407,\n",
      "                                                           422,\n",
      "                                                           428,\n",
      "                                                           448,\n",
      "                                                           452,\n",
      "                                                           468,\n",
      "                                                           470,\n",
      "                                                           475,\n",
      "                                                           484,\n",
      "                                                           485,\n",
      "                                                           492,\n",
      "                                                           515,\n",
      "                                                           526,\n",
      "                                                           539,\n",
      "                                                           542,\n",
      "                                                           548,\n",
      "                                                           587,\n",
      "                                                           592,\n",
      "                                                           597,\n",
      "                                                           611,\n",
      "                                                           625,\n",
      "                                                           638,\n",
      "                                                           658,\n",
      "                                                           685,\n",
      "                                                           692,\n",
      "                                                           697,\n",
      "                                                           702,\n",
      "                                                           718,\n",
      "                                                           737,\n",
      "                                                           739,\n",
      "                                                           741,\n",
      "                                                           749,\n",
      "                                                           760,\n",
      "                                                           776,\n",
      "                                                           783,\n",
      "                                                           790,\n",
      "                                                           793,\n",
      "                                                           800,\n",
      "                                                           801,\n",
      "                                                           820,\n",
      "                                                           825,\n",
      "                                                           833,\n",
      "                                                           836,\n",
      "                                                           838,\n",
      "                                                           853,\n",
      "                                                           875,\n",
      "                                                           879,\n",
      "                                                           885,\n",
      "                                                           888],\n",
      "                                          'diff_ratio': 1.560297227443609e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.15.mlp.down_proj.weight': {'diff_count': 62,\n",
      "                                          'diff_indices': [16,\n",
      "                                                           66,\n",
      "                                                           74,\n",
      "                                                           84,\n",
      "                                                           92,\n",
      "                                                           110,\n",
      "                                                           113,\n",
      "                                                           120,\n",
      "                                                           134,\n",
      "                                                           135,\n",
      "                                                           175,\n",
      "                                                           177,\n",
      "                                                           213,\n",
      "                                                           217,\n",
      "                                                           244,\n",
      "                                                           259,\n",
      "                                                           274,\n",
      "                                                           307,\n",
      "                                                           333,\n",
      "                                                           358,\n",
      "                                                           359,\n",
      "                                                           387,\n",
      "                                                           399,\n",
      "                                                           401,\n",
      "                                                           411,\n",
      "                                                           414,\n",
      "                                                           436,\n",
      "                                                           443,\n",
      "                                                           454,\n",
      "                                                           473,\n",
      "                                                           490,\n",
      "                                                           526,\n",
      "                                                           556,\n",
      "                                                           568,\n",
      "                                                           580,\n",
      "                                                           593,\n",
      "                                                           594,\n",
      "                                                           596,\n",
      "                                                           606,\n",
      "                                                           607,\n",
      "                                                           629,\n",
      "                                                           639,\n",
      "                                                           640,\n",
      "                                                           642,\n",
      "                                                           656,\n",
      "                                                           661,\n",
      "                                                           687,\n",
      "                                                           696,\n",
      "                                                           697,\n",
      "                                                           714,\n",
      "                                                           721,\n",
      "                                                           735,\n",
      "                                                           741,\n",
      "                                                           770,\n",
      "                                                           811,\n",
      "                                                           834,\n",
      "                                                           839,\n",
      "                                                           847,\n",
      "                                                           853,\n",
      "                                                           857,\n",
      "                                                           863,\n",
      "                                                           885],\n",
      "                                          'diff_ratio': 1.4226239426691729e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.16.mlp.down_proj.weight': {'diff_count': 55,\n",
      "                                          'diff_indices': [9,\n",
      "                                                           15,\n",
      "                                                           28,\n",
      "                                                           84,\n",
      "                                                           103,\n",
      "                                                           113,\n",
      "                                                           143,\n",
      "                                                           145,\n",
      "                                                           151,\n",
      "                                                           171,\n",
      "                                                           184,\n",
      "                                                           199,\n",
      "                                                           207,\n",
      "                                                           222,\n",
      "                                                           242,\n",
      "                                                           296,\n",
      "                                                           301,\n",
      "                                                           317,\n",
      "                                                           325,\n",
      "                                                           331,\n",
      "                                                           358,\n",
      "                                                           399,\n",
      "                                                           410,\n",
      "                                                           412,\n",
      "                                                           478,\n",
      "                                                           487,\n",
      "                                                           490,\n",
      "                                                           504,\n",
      "                                                           506,\n",
      "                                                           512,\n",
      "                                                           515,\n",
      "                                                           533,\n",
      "                                                           540,\n",
      "                                                           555,\n",
      "                                                           569,\n",
      "                                                           598,\n",
      "                                                           607,\n",
      "                                                           611,\n",
      "                                                           631,\n",
      "                                                           640,\n",
      "                                                           641,\n",
      "                                                           679,\n",
      "                                                           722,\n",
      "                                                           739,\n",
      "                                                           749,\n",
      "                                                           760,\n",
      "                                                           762,\n",
      "                                                           788,\n",
      "                                                           800,\n",
      "                                                           832,\n",
      "                                                           838,\n",
      "                                                           848,\n",
      "                                                           861,\n",
      "                                                           862,\n",
      "                                                           867],\n",
      "                                          'diff_ratio': 1.2620051104323308e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.17.mlp.down_proj.weight': {'diff_count': 48,\n",
      "                                          'diff_indices': [23,\n",
      "                                                           37,\n",
      "                                                           46,\n",
      "                                                           53,\n",
      "                                                           57,\n",
      "                                                           60,\n",
      "                                                           68,\n",
      "                                                           87,\n",
      "                                                           128,\n",
      "                                                           161,\n",
      "                                                           162,\n",
      "                                                           164,\n",
      "                                                           175,\n",
      "                                                           198,\n",
      "                                                           200,\n",
      "                                                           208,\n",
      "                                                           236,\n",
      "                                                           256,\n",
      "                                                           271,\n",
      "                                                           284,\n",
      "                                                           308,\n",
      "                                                           318,\n",
      "                                                           370,\n",
      "                                                           399,\n",
      "                                                           446,\n",
      "                                                           490,\n",
      "                                                           493,\n",
      "                                                           513,\n",
      "                                                           540,\n",
      "                                                           555,\n",
      "                                                           563,\n",
      "                                                           593,\n",
      "                                                           610,\n",
      "                                                           611,\n",
      "                                                           657,\n",
      "                                                           663,\n",
      "                                                           682,\n",
      "                                                           684,\n",
      "                                                           687,\n",
      "                                                           690,\n",
      "                                                           692,\n",
      "                                                           698,\n",
      "                                                           731,\n",
      "                                                           739,\n",
      "                                                           800,\n",
      "                                                           832,\n",
      "                                                           856,\n",
      "                                                           892],\n",
      "                                          'diff_ratio': 1.1013862781954887e-05,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.18.mlp.down_proj.weight': {'diff_count': 41,\n",
      "                                          'diff_indices': [4,\n",
      "                                                           11,\n",
      "                                                           15,\n",
      "                                                           51,\n",
      "                                                           52,\n",
      "                                                           85,\n",
      "                                                           139,\n",
      "                                                           143,\n",
      "                                                           161,\n",
      "                                                           175,\n",
      "                                                           189,\n",
      "                                                           213,\n",
      "                                                           216,\n",
      "                                                           221,\n",
      "                                                           246,\n",
      "                                                           261,\n",
      "                                                           269,\n",
      "                                                           308,\n",
      "                                                           313,\n",
      "                                                           329,\n",
      "                                                           365,\n",
      "                                                           394,\n",
      "                                                           395,\n",
      "                                                           402,\n",
      "                                                           470,\n",
      "                                                           473,\n",
      "                                                           481,\n",
      "                                                           490,\n",
      "                                                           501,\n",
      "                                                           611,\n",
      "                                                           643,\n",
      "                                                           654,\n",
      "                                                           658,\n",
      "                                                           692,\n",
      "                                                           774,\n",
      "                                                           800,\n",
      "                                                           805,\n",
      "                                                           821,\n",
      "                                                           822,\n",
      "                                                           829,\n",
      "                                                           865],\n",
      "                                          'diff_ratio': 9.407674459586466e-06,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.19.mlp.down_proj.weight': {'diff_count': 34,\n",
      "                                          'diff_indices': [62,\n",
      "                                                           68,\n",
      "                                                           85,\n",
      "                                                           86,\n",
      "                                                           139,\n",
      "                                                           168,\n",
      "                                                           170,\n",
      "                                                           228,\n",
      "                                                           256,\n",
      "                                                           259,\n",
      "                                                           267,\n",
      "                                                           327,\n",
      "                                                           348,\n",
      "                                                           451,\n",
      "                                                           457,\n",
      "                                                           459,\n",
      "                                                           460,\n",
      "                                                           487,\n",
      "                                                           540,\n",
      "                                                           572,\n",
      "                                                           592,\n",
      "                                                           611,\n",
      "                                                           616,\n",
      "                                                           633,\n",
      "                                                           653,\n",
      "                                                           667,\n",
      "                                                           682,\n",
      "                                                           687,\n",
      "                                                           689,\n",
      "                                                           765,\n",
      "                                                           774,\n",
      "                                                           778,\n",
      "                                                           800,\n",
      "                                                           839],\n",
      "                                          'diff_ratio': 7.801486137218045e-06,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.2.mlp.down_proj.weight': {'diff_count': 20,\n",
      "                                         'diff_indices': [17,\n",
      "                                                          26,\n",
      "                                                          34,\n",
      "                                                          62,\n",
      "                                                          78,\n",
      "                                                          208,\n",
      "                                                          249,\n",
      "                                                          328,\n",
      "                                                          433,\n",
      "                                                          438,\n",
      "                                                          494,\n",
      "                                                          625,\n",
      "                                                          648,\n",
      "                                                          674,\n",
      "                                                          693,\n",
      "                                                          787,\n",
      "                                                          797,\n",
      "                                                          800,\n",
      "                                                          814,\n",
      "                                                          818],\n",
      "                                         'diff_ratio': 4.589109492481203e-06,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.20.mlp.down_proj.weight': {'diff_count': 27,\n",
      "                                          'diff_indices': [50,\n",
      "                                                           54,\n",
      "                                                           85,\n",
      "                                                           90,\n",
      "                                                           98,\n",
      "                                                           136,\n",
      "                                                           143,\n",
      "                                                           279,\n",
      "                                                           288,\n",
      "                                                           292,\n",
      "                                                           307,\n",
      "                                                           327,\n",
      "                                                           333,\n",
      "                                                           421,\n",
      "                                                           435,\n",
      "                                                           460,\n",
      "                                                           493,\n",
      "                                                           500,\n",
      "                                                           533,\n",
      "                                                           540,\n",
      "                                                           567,\n",
      "                                                           690,\n",
      "                                                           735,\n",
      "                                                           765,\n",
      "                                                           775,\n",
      "                                                           800,\n",
      "                                                           867],\n",
      "                                          'diff_ratio': 6.195297814849624e-06,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.21.mlp.down_proj.weight': {'diff_count': 20,\n",
      "                                          'diff_indices': [4,\n",
      "                                                           60,\n",
      "                                                           79,\n",
      "                                                           85,\n",
      "                                                           143,\n",
      "                                                           186,\n",
      "                                                           197,\n",
      "                                                           268,\n",
      "                                                           293,\n",
      "                                                           333,\n",
      "                                                           386,\n",
      "                                                           401,\n",
      "                                                           444,\n",
      "                                                           483,\n",
      "                                                           490,\n",
      "                                                           566,\n",
      "                                                           783,\n",
      "                                                           800,\n",
      "                                                           829,\n",
      "                                                           861],\n",
      "                                          'diff_ratio': 4.589109492481203e-06,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.22.mlp.down_proj.weight': {'diff_count': 13,\n",
      "                                          'diff_indices': [15,\n",
      "                                                           36,\n",
      "                                                           200,\n",
      "                                                           232,\n",
      "                                                           239,\n",
      "                                                           241,\n",
      "                                                           285,\n",
      "                                                           333,\n",
      "                                                           429,\n",
      "                                                           617,\n",
      "                                                           646,\n",
      "                                                           718,\n",
      "                                                           861],\n",
      "                                          'diff_ratio': 2.982921170112782e-06,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.23.mlp.down_proj.weight': {'diff_count': 6,\n",
      "                                          'diff_indices': [75,\n",
      "                                                           208,\n",
      "                                                           241,\n",
      "                                                           333,\n",
      "                                                           665,\n",
      "                                                           854],\n",
      "                                          'diff_ratio': 1.3767328477443608e-06,\n",
      "                                          'shape': (896, 4864)},\n",
      " 'model.layers.3.mlp.down_proj.weight': {'diff_count': 27,\n",
      "                                         'diff_indices': [28,\n",
      "                                                          62,\n",
      "                                                          76,\n",
      "                                                          106,\n",
      "                                                          108,\n",
      "                                                          135,\n",
      "                                                          144,\n",
      "                                                          261,\n",
      "                                                          288,\n",
      "                                                          289,\n",
      "                                                          294,\n",
      "                                                          328,\n",
      "                                                          442,\n",
      "                                                          476,\n",
      "                                                          483,\n",
      "                                                          490,\n",
      "                                                          495,\n",
      "                                                          541,\n",
      "                                                          570,\n",
      "                                                          587,\n",
      "                                                          588,\n",
      "                                                          679,\n",
      "                                                          740,\n",
      "                                                          767,\n",
      "                                                          800,\n",
      "                                                          802,\n",
      "                                                          861],\n",
      "                                         'diff_ratio': 6.195297814849624e-06,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.4.mlp.down_proj.weight': {'diff_count': 34,\n",
      "                                         'diff_indices': [23,\n",
      "                                                          33,\n",
      "                                                          53,\n",
      "                                                          60,\n",
      "                                                          78,\n",
      "                                                          83,\n",
      "                                                          93,\n",
      "                                                          106,\n",
      "                                                          137,\n",
      "                                                          173,\n",
      "                                                          184,\n",
      "                                                          285,\n",
      "                                                          297,\n",
      "                                                          380,\n",
      "                                                          400,\n",
      "                                                          421,\n",
      "                                                          422,\n",
      "                                                          443,\n",
      "                                                          447,\n",
      "                                                          464,\n",
      "                                                          485,\n",
      "                                                          525,\n",
      "                                                          526,\n",
      "                                                          536,\n",
      "                                                          640,\n",
      "                                                          646,\n",
      "                                                          686,\n",
      "                                                          719,\n",
      "                                                          744,\n",
      "                                                          786,\n",
      "                                                          787,\n",
      "                                                          802,\n",
      "                                                          815,\n",
      "                                                          827],\n",
      "                                         'diff_ratio': 7.801486137218045e-06,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.5.mlp.down_proj.weight': {'diff_count': 41,\n",
      "                                         'diff_indices': [3,\n",
      "                                                          18,\n",
      "                                                          60,\n",
      "                                                          90,\n",
      "                                                          132,\n",
      "                                                          170,\n",
      "                                                          232,\n",
      "                                                          241,\n",
      "                                                          249,\n",
      "                                                          267,\n",
      "                                                          271,\n",
      "                                                          294,\n",
      "                                                          297,\n",
      "                                                          324,\n",
      "                                                          363,\n",
      "                                                          364,\n",
      "                                                          365,\n",
      "                                                          376,\n",
      "                                                          438,\n",
      "                                                          442,\n",
      "                                                          466,\n",
      "                                                          490,\n",
      "                                                          525,\n",
      "                                                          543,\n",
      "                                                          548,\n",
      "                                                          550,\n",
      "                                                          593,\n",
      "                                                          603,\n",
      "                                                          642,\n",
      "                                                          646,\n",
      "                                                          649,\n",
      "                                                          655,\n",
      "                                                          693,\n",
      "                                                          765,\n",
      "                                                          770,\n",
      "                                                          776,\n",
      "                                                          824,\n",
      "                                                          837,\n",
      "                                                          863,\n",
      "                                                          877,\n",
      "                                                          880],\n",
      "                                         'diff_ratio': 9.407674459586466e-06,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.6.mlp.down_proj.weight': {'diff_count': 48,\n",
      "                                         'diff_indices': [16,\n",
      "                                                          25,\n",
      "                                                          44,\n",
      "                                                          75,\n",
      "                                                          135,\n",
      "                                                          159,\n",
      "                                                          174,\n",
      "                                                          197,\n",
      "                                                          208,\n",
      "                                                          249,\n",
      "                                                          251,\n",
      "                                                          254,\n",
      "                                                          274,\n",
      "                                                          275,\n",
      "                                                          285,\n",
      "                                                          293,\n",
      "                                                          345,\n",
      "                                                          361,\n",
      "                                                          376,\n",
      "                                                          381,\n",
      "                                                          388,\n",
      "                                                          408,\n",
      "                                                          412,\n",
      "                                                          423,\n",
      "                                                          490,\n",
      "                                                          495,\n",
      "                                                          509,\n",
      "                                                          517,\n",
      "                                                          523,\n",
      "                                                          543,\n",
      "                                                          544,\n",
      "                                                          553,\n",
      "                                                          570,\n",
      "                                                          577,\n",
      "                                                          612,\n",
      "                                                          637,\n",
      "                                                          658,\n",
      "                                                          701,\n",
      "                                                          710,\n",
      "                                                          749,\n",
      "                                                          770,\n",
      "                                                          777,\n",
      "                                                          794,\n",
      "                                                          801,\n",
      "                                                          834,\n",
      "                                                          837,\n",
      "                                                          841,\n",
      "                                                          867],\n",
      "                                         'diff_ratio': 1.1013862781954887e-05,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.7.mlp.down_proj.weight': {'diff_count': 55,\n",
      "                                         'diff_indices': [4,\n",
      "                                                          5,\n",
      "                                                          53,\n",
      "                                                          62,\n",
      "                                                          64,\n",
      "                                                          66,\n",
      "                                                          74,\n",
      "                                                          85,\n",
      "                                                          97,\n",
      "                                                          126,\n",
      "                                                          136,\n",
      "                                                          146,\n",
      "                                                          151,\n",
      "                                                          156,\n",
      "                                                          207,\n",
      "                                                          208,\n",
      "                                                          218,\n",
      "                                                          251,\n",
      "                                                          279,\n",
      "                                                          287,\n",
      "                                                          293,\n",
      "                                                          296,\n",
      "                                                          298,\n",
      "                                                          301,\n",
      "                                                          400,\n",
      "                                                          411,\n",
      "                                                          441,\n",
      "                                                          475,\n",
      "                                                          503,\n",
      "                                                          541,\n",
      "                                                          542,\n",
      "                                                          592,\n",
      "                                                          620,\n",
      "                                                          632,\n",
      "                                                          633,\n",
      "                                                          636,\n",
      "                                                          644,\n",
      "                                                          648,\n",
      "                                                          656,\n",
      "                                                          692,\n",
      "                                                          722,\n",
      "                                                          731,\n",
      "                                                          735,\n",
      "                                                          779,\n",
      "                                                          784,\n",
      "                                                          787,\n",
      "                                                          790,\n",
      "                                                          795,\n",
      "                                                          802,\n",
      "                                                          837,\n",
      "                                                          844,\n",
      "                                                          848,\n",
      "                                                          859,\n",
      "                                                          863,\n",
      "                                                          895],\n",
      "                                         'diff_ratio': 1.2620051104323308e-05,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.8.mlp.down_proj.weight': {'diff_count': 62,\n",
      "                                         'diff_indices': [11,\n",
      "                                                          38,\n",
      "                                                          62,\n",
      "                                                          68,\n",
      "                                                          96,\n",
      "                                                          99,\n",
      "                                                          107,\n",
      "                                                          115,\n",
      "                                                          125,\n",
      "                                                          133,\n",
      "                                                          141,\n",
      "                                                          144,\n",
      "                                                          145,\n",
      "                                                          146,\n",
      "                                                          172,\n",
      "                                                          209,\n",
      "                                                          242,\n",
      "                                                          244,\n",
      "                                                          263,\n",
      "                                                          267,\n",
      "                                                          278,\n",
      "                                                          302,\n",
      "                                                          303,\n",
      "                                                          348,\n",
      "                                                          374,\n",
      "                                                          375,\n",
      "                                                          421,\n",
      "                                                          441,\n",
      "                                                          446,\n",
      "                                                          459,\n",
      "                                                          483,\n",
      "                                                          490,\n",
      "                                                          494,\n",
      "                                                          508,\n",
      "                                                          511,\n",
      "                                                          538,\n",
      "                                                          542,\n",
      "                                                          544,\n",
      "                                                          547,\n",
      "                                                          550,\n",
      "                                                          571,\n",
      "                                                          588,\n",
      "                                                          592,\n",
      "                                                          595,\n",
      "                                                          618,\n",
      "                                                          625,\n",
      "                                                          631,\n",
      "                                                          634,\n",
      "                                                          637,\n",
      "                                                          652,\n",
      "                                                          687,\n",
      "                                                          695,\n",
      "                                                          703,\n",
      "                                                          760,\n",
      "                                                          767,\n",
      "                                                          820,\n",
      "                                                          833,\n",
      "                                                          837,\n",
      "                                                          840,\n",
      "                                                          848,\n",
      "                                                          853,\n",
      "                                                          884],\n",
      "                                         'diff_ratio': 1.4226239426691729e-05,\n",
      "                                         'shape': (896, 4864)},\n",
      " 'model.layers.9.mlp.down_proj.weight': {'diff_count': 68,\n",
      "                                         'diff_indices': [5,\n",
      "                                                          7,\n",
      "                                                          13,\n",
      "                                                          26,\n",
      "                                                          41,\n",
      "                                                          49,\n",
      "                                                          62,\n",
      "                                                          74,\n",
      "                                                          92,\n",
      "                                                          121,\n",
      "                                                          123,\n",
      "                                                          129,\n",
      "                                                          143,\n",
      "                                                          163,\n",
      "                                                          168,\n",
      "                                                          236,\n",
      "                                                          247,\n",
      "                                                          252,\n",
      "                                                          269,\n",
      "                                                          281,\n",
      "                                                          297,\n",
      "                                                          327,\n",
      "                                                          333,\n",
      "                                                          342,\n",
      "                                                          345,\n",
      "                                                          347,\n",
      "                                                          348,\n",
      "                                                          355,\n",
      "                                                          365,\n",
      "                                                          366,\n",
      "                                                          371,\n",
      "                                                          380,\n",
      "                                                          383,\n",
      "                                                          389,\n",
      "                                                          432,\n",
      "                                                          455,\n",
      "                                                          489,\n",
      "                                                          508,\n",
      "                                                          515,\n",
      "                                                          535,\n",
      "                                                          538,\n",
      "                                                          546,\n",
      "                                                          558,\n",
      "                                                          571,\n",
      "                                                          587,\n",
      "                                                          596,\n",
      "                                                          600,\n",
      "                                                          654,\n",
      "                                                          658,\n",
      "                                                          687,\n",
      "                                                          692,\n",
      "                                                          693,\n",
      "                                                          694,\n",
      "                                                          700,\n",
      "                                                          721,\n",
      "                                                          735,\n",
      "                                                          742,\n",
      "                                                          747,\n",
      "                                                          749,\n",
      "                                                          762,\n",
      "                                                          796,\n",
      "                                                          797,\n",
      "                                                          825,\n",
      "                                                          862,\n",
      "                                                          870,\n",
      "                                                          873,\n",
      "                                                          879,\n",
      "                                                          883],\n",
      "                                         'diff_ratio': 1.560297227443609e-05,\n",
      "                                         'shape': (896, 4864)}}\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def compare_mlp_params(model1, model2, mlp_layers: Union[str, List[str]]) -> dict:\n",
    "    \"\"\"比较多个MLP层的参数差异\n",
    "\n",
    "    Args:\n",
    "        mlp_layers: 支持以下格式：\n",
    "            - 单个层模式: \"transformer.h.0.mlp\"\n",
    "            - 多个层模式: [\"transformer.h.0.mlp\", \"transformer.h.5.mlp\"]\n",
    "    \"\"\"\n",
    "    # 统一处理为列表格式\n",
    "    if isinstance(mlp_layers, str):\n",
    "        target_patterns = [mlp_layers]\n",
    "    else:\n",
    "        target_patterns = mlp_layers\n",
    "\n",
    "    # 多模式参数提取\n",
    "    def filter_params(model):\n",
    "        return {\n",
    "            name: param\n",
    "            for name, param in model.named_parameters()\n",
    "            if any(pattern in name for pattern in target_patterns)\n",
    "        }\n",
    "\n",
    "    params1 = filter_params(model1)\n",
    "    params2 = filter_params(model2)\n",
    "\n",
    "    # 结构一致性检查\n",
    "    if params1.keys() != params2.keys():\n",
    "        missing_in_1 = set(params2.keys()) - set(params1.keys())\n",
    "        missing_in_2 = set(params1.keys()) - set(params2.keys())\n",
    "        raise ValueError(\n",
    "            f\"模型结构不一致\\n\"\n",
    "            f\"Model1缺失层: {list(missing_in_1)}\\n\"\n",
    "            f\"Model2缺失层: {list(missing_in_2)}\"\n",
    "        )\n",
    "\n",
    "    differences = {}\n",
    "\n",
    "    for name in params1:\n",
    "        p1, p2 = params1[name].cpu(), params2[name].cpu()\n",
    "\n",
    "        if p1.shape != p2.shape:\n",
    "            raise ValueError(f\"形状不匹配: {name} | {p1.shape} vs {p2.shape}\")\n",
    "\n",
    "        if not torch.equal(p1, p2):\n",
    "            diff_mask = ~torch.isclose(p1, p2, rtol=1e-5, atol=1e-8)\n",
    "            diff_indices = torch.unique(torch.nonzero(diff_mask)[:, 0])\n",
    "\n",
    "            differences[name] = {\n",
    "                \"shape\": tuple(p1.shape),\n",
    "                \"diff_count\": diff_indices.size(0),\n",
    "                \"diff_ratio\": diff_indices.size(0) / p1.numel(),\n",
    "                \"diff_indices\": diff_indices.tolist(),\n",
    "            }\n",
    "\n",
    "    return differences\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 单层比较\n",
    "    # diff = compare_mlp_params(model1_path, model2_path, \"transformer.h.0.mlp\")\n",
    "    model1_path = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    model2_path = \"/cache/models/loki_reranker_qwen2_5-0-5b-5_real\"\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        model1_path, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(\n",
    "        model2_path, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    # for name, param in model1.named_parameters():\n",
    "    #     print(name)\n",
    "    target_modules = []\n",
    "    for idx, layer in enumerate(model1.model.layers):\n",
    "        module_str = f\"model.layers.{idx}.mlp.down_proj.weight\"\n",
    "        target_modules.append(module_str)\n",
    "    # 多层比较\n",
    "    diff = compare_mlp_params(model1, model2, mlp_layers=target_modules)\n",
    "    pprint(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打印模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(151936, 896)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"/cache/models/loki_reranker_qwen2_5-0-5b-5_real\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "with safe_open(\"/cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688/model.safetensors\", framework=\"pt\") as f:\n",
    "    print(f.keys())  # 直接输出所有权重键名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 还原模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688 were not used when initializing Qwen2ForCausalLM: {'model.layers.0.mlp.down_proj.active_part.weight', 'model.layers.14.mlp.down_proj.fixed_part.weight', 'model.layers.7.mlp.down_proj.active_part.weight', 'model.layers.21.mlp.down_proj.active_part.weight', 'model.layers.4.mlp.down_proj.fixed_part.weight', 'model.layers.5.mlp.down_proj.fixed_part.weight', 'model.layers.2.mlp.down_proj.active_part.weight', 'model.layers.6.mlp.down_proj.fixed_part.weight', 'model.layers.1.mlp.down_proj.active_part.weight', 'model.layers.8.mlp.down_proj.active_part.weight', 'model.layers.18.mlp.down_proj.active_part.weight', 'model.layers.22.mlp.down_proj.fixed_part.weight', 'model.layers.3.mlp.down_proj.fixed_part.weight', 'model.layers.15.mlp.down_proj.fixed_part.weight', 'model.layers.16.mlp.down_proj.active_part.weight', 'model.layers.23.mlp.down_proj.fixed_part.weight', 'model.layers.17.mlp.down_proj.active_part.weight', 'model.layers.20.mlp.down_proj.fixed_part.weight', 'model.layers.6.mlp.down_proj.active_part.weight', 'model.layers.16.mlp.down_proj.fixed_part.weight', 'model.layers.9.mlp.down_proj.active_part.weight', 'model.layers.0.mlp.down_proj.fixed_part.weight', 'model.layers.3.mlp.down_proj.active_part.weight', 'model.layers.12.mlp.down_proj.active_part.weight', 'model.layers.20.mlp.down_proj.active_part.weight', 'model.layers.23.mlp.down_proj.active_part.weight', 'model.layers.21.mlp.down_proj.fixed_part.weight', 'model.layers.18.mlp.down_proj.fixed_part.weight', 'model.layers.22.mlp.down_proj.active_part.weight', 'model.layers.12.mlp.down_proj.fixed_part.weight', 'model.layers.4.mlp.down_proj.active_part.weight', 'model.layers.13.mlp.down_proj.fixed_part.weight', 'model.layers.11.mlp.down_proj.active_part.weight', 'model.layers.10.mlp.down_proj.active_part.weight', 'model.layers.8.mlp.down_proj.fixed_part.weight', 'model.layers.13.mlp.down_proj.active_part.weight', 'model.layers.14.mlp.down_proj.active_part.weight', 'model.layers.1.mlp.down_proj.fixed_part.weight', 'model.layers.19.mlp.down_proj.fixed_part.weight', 'model.layers.17.mlp.down_proj.fixed_part.weight', 'model.layers.2.mlp.down_proj.fixed_part.weight', 'model.layers.9.mlp.down_proj.fixed_part.weight', 'model.layers.19.mlp.down_proj.active_part.weight', 'model.layers.10.mlp.down_proj.fixed_part.weight', 'model.layers.5.mlp.down_proj.active_part.weight', 'model.layers.11.mlp.down_proj.fixed_part.weight', 'model.layers.7.mlp.down_proj.fixed_part.weight', 'model.layers.15.mlp.down_proj.active_part.weight'}\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at /cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers import AutoModelForCausalLM\n",
    "from src.module.loki_linear import LoKILinear\n",
    "import json\n",
    "import torch\n",
    "\n",
    "checkpoint_path = (\n",
    "    \"/cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688/model.safetensors\"\n",
    ")\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/5.json\"\n",
    "target_model = \"/cache/models/loki_reranker_qwen2_5-0-5b-5/checkpoint-456688\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "trainable_neurons = list(data)\n",
    "# 重新初始化原始模型结构\n",
    "original_model = AutoModelForCausalLM.from_pretrained(target_model)\n",
    "\n",
    "\n",
    "def merge_loki_weights(loki_layer, original_linear):\n",
    "    # 合并权重矩阵\n",
    "    merged_weight = torch.zeros_like(original_linear.weight.data)\n",
    "    merged_weight[loki_layer.active_pos] = loki_layer.active_part.weight.data\n",
    "    merged_weight[loki_layer.fixed_pos] = loki_layer.fixed_part.weight.data\n",
    "\n",
    "    # 合并偏置项\n",
    "    if original_linear.bias is not None:\n",
    "        merged_bias = torch.zeros_like(original_linear.bias.data)\n",
    "        merged_bias[loki_layer.active_pos] = loki_layer.active_bias.data\n",
    "        merged_bias[loki_layer.fixed_pos] = loki_layer.fixed_bias.data\n",
    "\n",
    "    # 加载参数到原始层\n",
    "    original_linear.weight.data.copy_(merged_weight)\n",
    "    if original_linear.bias is not None:\n",
    "        original_linear.bias.data.copy_(merged_bias)\n",
    "\n",
    "\n",
    "# 加载检查点文件\n",
    "\n",
    "# 遍历所有层还原参数\n",
    "for layer_idx in range(original_model.config.num_hidden_layers):\n",
    "    # 获取当前层的原始结构\n",
    "    original_down_proj = original_model.model.layers[layer_idx].mlp.down_proj\n",
    "\n",
    "    # 加载LoKI层参数\n",
    "    with safe_open(checkpoint_path, framework=\"pt\") as f:\n",
    "        # 创建临时LoKI层用于加载参数\n",
    "        loki_layer = LoKILinear(\n",
    "            original_down_proj, target_neurons=trainable_neurons[layer_idx]\n",
    "        )\n",
    "        loki_layer.load_state_dict(\n",
    "            {\n",
    "                \"active_part.weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.active_part.weight\"\n",
    "                ),\n",
    "                \"fixed_part.weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.fixed_part.weight\"\n",
    "                ),\n",
    "                # \"active_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.active_bias\"\n",
    "                # ),\n",
    "                # \"fixed_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.fixed_bias\"\n",
    "                # ),\n",
    "            },\n",
    "            strict=True,\n",
    "        )\n",
    "        weight = f.get_tensor(f\"model.layers.{layer_idx}.mlp.down_proj.active_part.weight\")\n",
    "\n",
    "    # 合并参数到原始层\n",
    "    merge_loki_weights(loki_layer, original_down_proj)\n",
    "\n",
    "# 保存还原后的模型\n",
    "original_model.save_pretrained(\"/cache/models/loki_reranker_qwen2_5-0-5b-5_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at /cache/models/loki_reranker_qwen2_5-0-5b-40 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-26 23:49:59,751] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/compiler_compat/ld: warning: librt.so.1, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/root/miniconda3/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/root/miniconda3/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/root/miniconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int*@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long*@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/root/miniconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers import AutoModelForCausalLM\n",
    "from src.loki.loki_linear import LoKILinear\n",
    "import json\n",
    "import torch\n",
    "\n",
    "checkpoint_path = (\n",
    "    \"/cache/models/loki_reranker_qwen2_5-0-5b-40/model.safetensors\"\n",
    ")\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/40.json\"\n",
    "target_model = \"/cache/models/loki_reranker_qwen2_5-0-5b-40\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "trainable_neurons = list(data)\n",
    "# 重新初始化原始模型结构\n",
    "original_model = AutoModelForCausalLM.from_pretrained(target_model)\n",
    "\n",
    "\n",
    "def merge_loki_weights(loki_layer, original_linear):\n",
    "    # 合并权重矩阵\n",
    "    merged_weight = torch.zeros_like(original_linear.weight.data)\n",
    "    merged_weight[loki_layer.active_pos] = loki_layer.active_weight.data\n",
    "    merged_weight[loki_layer.fixed_pos] = loki_layer.fixed_weight.data\n",
    "\n",
    "    # 合并偏置项\n",
    "    if original_linear.bias is not None:\n",
    "        merged_bias = torch.zeros_like(original_linear.bias.data)\n",
    "        merged_bias[loki_layer.active_pos] = loki_layer.active_bias.data\n",
    "        merged_bias[loki_layer.fixed_pos] = loki_layer.fixed_bias.data\n",
    "\n",
    "    # 加载参数到原始层\n",
    "    original_linear.weight.data.copy_(merged_weight)\n",
    "    if original_linear.bias is not None:\n",
    "        original_linear.bias.data.copy_(merged_bias)\n",
    "\n",
    "\n",
    "# 加载检查点文件\n",
    "\n",
    "# 遍历所有层还原参数\n",
    "for layer_idx in range(original_model.config.num_hidden_layers):\n",
    "    # 获取当前层的原始结构\n",
    "    original_down_proj = original_model.model.layers[layer_idx].mlp.down_proj\n",
    "\n",
    "    # 加载LoKI层参数\n",
    "    with safe_open(checkpoint_path, framework=\"pt\") as f:\n",
    "        # 创建临时LoKI层用于加载参数\n",
    "        loki_layer = LoKILinear(\n",
    "            original_down_proj, target_neurons=trainable_neurons[layer_idx]\n",
    "        )\n",
    "        loki_layer.load_state_dict(\n",
    "            {\n",
    "                \"active_weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.active_weight\"\n",
    "                ),\n",
    "                \"fixed_weight\": f.get_tensor(\n",
    "                    f\"model.layers.{layer_idx}.mlp.down_proj.fixed_weight\"\n",
    "                ),\n",
    "                \"index_map\" : loki_layer.index_map\n",
    "                # \"active_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.active_bias\"\n",
    "                # ),\n",
    "                # \"fixed_bias\": f.get_tensor(\n",
    "                #     f\"model.layers.{layer_idx}.mlp.down_proj.fixed_bias\"\n",
    "                # ),\n",
    "            },\n",
    "            strict=True,\n",
    "        )\n",
    "        weight = f.get_tensor(f\"model.layers.{layer_idx}.mlp.down_proj.active_weight\")\n",
    "\n",
    "    # 合并参数到原始层\n",
    "    merge_loki_weights(loki_layer, original_down_proj)\n",
    "\n",
    "# 保存还原后的模型\n",
    "original_model.save_pretrained(\"/cache/models/loki_reranker_qwen2_5-0-5b-40_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功替换层0\n",
      "成功替换层1\n",
      "成功替换层2\n",
      "成功替换层3\n",
      "成功替换层4\n",
      "成功替换层5\n",
      "成功替换层6\n",
      "成功替换层7\n",
      "成功替换层8\n",
      "成功替换层9\n",
      "成功替换层10\n",
      "成功替换层11\n",
      "成功替换层12\n",
      "成功替换层13\n",
      "成功替换层14\n",
      "成功替换层15\n",
      "成功替换层16\n",
      "成功替换层17\n",
      "成功替换层18\n",
      "成功替换层19\n",
      "成功替换层20\n",
      "成功替换层21\n",
      "成功替换层22\n",
      "成功替换层23\n",
      "LoKIQwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=6, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=890, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (1): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=13, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=883, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (2): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=20, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=876, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (3): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=27, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=869, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (4): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=34, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=862, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (5): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=41, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=855, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (6): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=48, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=848, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (7): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=55, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=841, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (8): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=62, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=834, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (9): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=68, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=828, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (10): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=75, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=821, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (11-12): 2 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=82, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=814, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (13): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=75, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=821, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (14): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=68, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=828, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (15): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=62, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=834, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (16): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=55, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=841, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (17): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=48, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=848, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (18): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=41, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=855, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (19): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=34, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=862, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (20): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=27, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=869, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (21): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=20, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=876, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (22): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=13, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=883, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "      (23): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear(\n",
      "            (active_part): Linear(in_features=4864, out_features=6, bias=False)\n",
      "            (fixed_part): Linear(in_features=4864, out_features=890, bias=False)\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from src.train.qwen_loki import LoKIQwen2ForCausalLM\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/5.json\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# custom_config = LoKIQwen2Config(\"Qwen/Qwen2.5-0.5B-Instruct\", target_neurons=data)\n",
    "model = LoKIQwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    target_neurons=data,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(model)\n",
    "# 验证第一层权重是否迁移\n",
    "loki_layer = model.model.layers[0].mlp.down_proj\n",
    "\n",
    "pretrained_weight = original_model.model.layers[0].mlp.down_proj.weight\n",
    "\n",
    "combined_weight = torch.zeros_like(pretrained_weight)\n",
    "combined_weight[loki_layer.active_pos] = loki_layer.active_part.weight.data\n",
    "combined_weight[loki_layer.fixed_pos] = loki_layer.fixed_part.weight.data\n",
    "print(torch.allclose(combined_weight, pretrained_weight, atol=1e-6))  # 应输出 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功替换层0\n",
      "成功替换层1\n",
      "成功替换层2\n",
      "成功替换层3\n",
      "成功替换层4\n",
      "成功替换层5\n",
      "成功替换层6\n",
      "成功替换层7\n",
      "成功替换层8\n",
      "成功替换层9\n",
      "成功替换层10\n",
      "成功替换层11\n",
      "成功替换层12\n",
      "成功替换层13\n",
      "成功替换层14\n",
      "成功替换层15\n",
      "成功替换层16\n",
      "成功替换层17\n",
      "成功替换层18\n",
      "成功替换层19\n",
      "成功替换层20\n",
      "成功替换层21\n",
      "成功替换层22\n",
      "成功替换层23\n"
     ]
    }
   ],
   "source": [
    "from src.loki.qwen_loki import LoKIQwen2ForCausalLM\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, Qwen2Config\n",
    "\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/5.json\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# custom_config = LoKIQwen2Config(\"Qwen/Qwen2.5-0.5B-Instruct\", target_neurons=data)\n",
    "model = LoKIQwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    target_neurons=data,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "LoKIQwen2ForCausalLM.register_for_auto_class(\"AutoModelForCausalLM\")\n",
    "\n",
    "# AutoModel.register(Qwen2Config, LoKIQwen2ForCausalLM)\n",
    "# LoKIQwen2ForCausalLM.register_for_auto_class(\"AutoModel\")\n",
    "model.save_pretrained(\"/cache/models/custom-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义模型加载测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Qwen2ForCausalLM.__init__() got an unexpected keyword argument 'target_neurons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(target_neurons_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/cache/models/Qwen2.5-3B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_neurons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4185\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4179\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4180\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4181\u001b[0m     )\n\u001b[1;32m   4183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   4184\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4185\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4187\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   4188\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: Qwen2ForCausalLM.__init__() got an unexpected keyword argument 'target_neurons'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "target_neurons_path = \"target_neurons/Qwen2.5-0.5B-Instruct/10.json\"\n",
    "\n",
    "with open(target_neurons_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/cache/models/Qwen2.5-3B-Instruct\",\n",
    "    target_neurons=data,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功替换层0\n",
      "成功替换层1\n",
      "成功替换层2\n",
      "成功替换层3\n",
      "成功替换层4\n",
      "成功替换层5\n",
      "成功替换层6\n",
      "成功替换层7\n",
      "成功替换层8\n",
      "成功替换层9\n",
      "成功替换层10\n",
      "成功替换层11\n",
      "成功替换层12\n",
      "成功替换层13\n",
      "成功替换层14\n",
      "成功替换层15\n",
      "成功替换层16\n",
      "成功替换层17\n",
      "成功替换层18\n",
      "成功替换层19\n",
      "成功替换层20\n",
      "成功替换层21\n",
      "成功替换层22\n",
      "成功替换层23\n",
      "Parameter: model.layers.0.mlp.down_proj.active_weight, Shape: torch.Size([13, 4864])\n",
      "Parameter: model.layers.1.mlp.down_proj.active_weight, Shape: torch.Size([27, 4864])\n",
      "Parameter: model.layers.2.mlp.down_proj.active_weight, Shape: torch.Size([41, 4864])\n",
      "Parameter: model.layers.3.mlp.down_proj.active_weight, Shape: torch.Size([55, 4864])\n",
      "Parameter: model.layers.4.mlp.down_proj.active_weight, Shape: torch.Size([68, 4864])\n",
      "Parameter: model.layers.5.mlp.down_proj.active_weight, Shape: torch.Size([82, 4864])\n",
      "Parameter: model.layers.6.mlp.down_proj.active_weight, Shape: torch.Size([96, 4864])\n",
      "Parameter: model.layers.7.mlp.down_proj.active_weight, Shape: torch.Size([110, 4864])\n",
      "Parameter: model.layers.8.mlp.down_proj.active_weight, Shape: torch.Size([124, 4864])\n",
      "Parameter: model.layers.9.mlp.down_proj.active_weight, Shape: torch.Size([137, 4864])\n",
      "Parameter: model.layers.10.mlp.down_proj.active_weight, Shape: torch.Size([151, 4864])\n",
      "Parameter: model.layers.11.mlp.down_proj.active_weight, Shape: torch.Size([165, 4864])\n",
      "Parameter: model.layers.12.mlp.down_proj.active_weight, Shape: torch.Size([165, 4864])\n",
      "Parameter: model.layers.13.mlp.down_proj.active_weight, Shape: torch.Size([151, 4864])\n",
      "Parameter: model.layers.14.mlp.down_proj.active_weight, Shape: torch.Size([137, 4864])\n",
      "Parameter: model.layers.15.mlp.down_proj.active_weight, Shape: torch.Size([124, 4864])\n",
      "Parameter: model.layers.16.mlp.down_proj.active_weight, Shape: torch.Size([110, 4864])\n",
      "Parameter: model.layers.17.mlp.down_proj.active_weight, Shape: torch.Size([96, 4864])\n",
      "Parameter: model.layers.18.mlp.down_proj.active_weight, Shape: torch.Size([82, 4864])\n",
      "Parameter: model.layers.19.mlp.down_proj.active_weight, Shape: torch.Size([68, 4864])\n",
      "Parameter: model.layers.20.mlp.down_proj.active_weight, Shape: torch.Size([55, 4864])\n",
      "Parameter: model.layers.21.mlp.down_proj.active_weight, Shape: torch.Size([41, 4864])\n",
      "Parameter: model.layers.22.mlp.down_proj.active_weight, Shape: torch.Size([27, 4864])\n",
      "Parameter: model.layers.23.mlp.down_proj.active_weight, Shape: torch.Size([13, 4864])\n",
      "Total Trainable Parameters: 10399232\n",
      "LoKIQwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear()\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/cache/models/custom-model-test\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载后Model还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功替换层0\n",
      "成功替换层1\n",
      "成功替换层2\n",
      "成功替换层3\n",
      "成功替换层4\n",
      "成功替换层5\n",
      "成功替换层6\n",
      "成功替换层7\n",
      "成功替换层8\n",
      "成功替换层9\n",
      "成功替换层10\n",
      "成功替换层11\n",
      "成功替换层12\n",
      "成功替换层13\n",
      "成功替换层14\n",
      "成功替换层15\n",
      "成功替换层16\n",
      "成功替换层17\n",
      "成功替换层18\n",
      "成功替换层19\n",
      "成功替换层20\n",
      "成功替换层21\n",
      "成功替换层22\n",
      "成功替换层23\n",
      "Parameter: model.layers.0.mlp.down_proj.active_weight, Shape: torch.Size([13, 4864])\n",
      "Parameter: model.layers.1.mlp.down_proj.active_weight, Shape: torch.Size([27, 4864])\n",
      "Parameter: model.layers.2.mlp.down_proj.active_weight, Shape: torch.Size([41, 4864])\n",
      "Parameter: model.layers.3.mlp.down_proj.active_weight, Shape: torch.Size([55, 4864])\n",
      "Parameter: model.layers.4.mlp.down_proj.active_weight, Shape: torch.Size([68, 4864])\n",
      "Parameter: model.layers.5.mlp.down_proj.active_weight, Shape: torch.Size([82, 4864])\n",
      "Parameter: model.layers.6.mlp.down_proj.active_weight, Shape: torch.Size([96, 4864])\n",
      "Parameter: model.layers.7.mlp.down_proj.active_weight, Shape: torch.Size([110, 4864])\n",
      "Parameter: model.layers.8.mlp.down_proj.active_weight, Shape: torch.Size([124, 4864])\n",
      "Parameter: model.layers.9.mlp.down_proj.active_weight, Shape: torch.Size([137, 4864])\n",
      "Parameter: model.layers.10.mlp.down_proj.active_weight, Shape: torch.Size([151, 4864])\n",
      "Parameter: model.layers.11.mlp.down_proj.active_weight, Shape: torch.Size([165, 4864])\n",
      "Parameter: model.layers.12.mlp.down_proj.active_weight, Shape: torch.Size([165, 4864])\n",
      "Parameter: model.layers.13.mlp.down_proj.active_weight, Shape: torch.Size([151, 4864])\n",
      "Parameter: model.layers.14.mlp.down_proj.active_weight, Shape: torch.Size([137, 4864])\n",
      "Parameter: model.layers.15.mlp.down_proj.active_weight, Shape: torch.Size([124, 4864])\n",
      "Parameter: model.layers.16.mlp.down_proj.active_weight, Shape: torch.Size([110, 4864])\n",
      "Parameter: model.layers.17.mlp.down_proj.active_weight, Shape: torch.Size([96, 4864])\n",
      "Parameter: model.layers.18.mlp.down_proj.active_weight, Shape: torch.Size([82, 4864])\n",
      "Parameter: model.layers.19.mlp.down_proj.active_weight, Shape: torch.Size([68, 4864])\n",
      "Parameter: model.layers.20.mlp.down_proj.active_weight, Shape: torch.Size([55, 4864])\n",
      "Parameter: model.layers.21.mlp.down_proj.active_weight, Shape: torch.Size([41, 4864])\n",
      "Parameter: model.layers.22.mlp.down_proj.active_weight, Shape: torch.Size([27, 4864])\n",
      "Parameter: model.layers.23.mlp.down_proj.active_weight, Shape: torch.Size([13, 4864])\n",
      "Total Trainable Parameters: 10399232\n",
      "LoKIQwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): LoKILinear()\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "成功还原层0的down_proj\n",
      "成功还原层1的down_proj\n",
      "成功还原层2的down_proj\n",
      "成功还原层3的down_proj\n",
      "成功还原层4的down_proj\n",
      "成功还原层5的down_proj\n",
      "成功还原层6的down_proj\n",
      "成功还原层7的down_proj\n",
      "成功还原层8的down_proj\n",
      "成功还原层9的down_proj\n",
      "成功还原层10的down_proj\n",
      "成功还原层11的down_proj\n",
      "成功还原层12的down_proj\n",
      "成功还原层13的down_proj\n",
      "成功还原层14的down_proj\n",
      "成功还原层15的down_proj\n",
      "成功还原层16的down_proj\n",
      "成功还原层17的down_proj\n",
      "成功还原层18的down_proj\n",
      "成功还原层19的down_proj\n",
      "成功还原层20的down_proj\n",
      "成功还原层21的down_proj\n",
      "成功还原层22的down_proj\n",
      "成功还原层23的down_proj\n",
      "LoKIQwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.loki.loki_linear import restore_original_linears\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/cache/models/custom-model-test\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)\n",
    "model = restore_original_linears(model)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
